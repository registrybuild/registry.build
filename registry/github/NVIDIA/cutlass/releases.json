[{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/234087117","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/234087117/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/234087117/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v4.1.0","id":234087117,"author":{"login":"hwu36","id":57973641,"node_id":"MDQ6VXNlcjU3OTczNjQx","avatar_url":"https://avatars.githubusercontent.com/u/57973641?v=4","gravatar_id":"","url":"https://api.github.com/users/hwu36","html_url":"https://github.com/hwu36","followers_url":"https://api.github.com/users/hwu36/followers","following_url":"https://api.github.com/users/hwu36/following{/other_user}","gists_url":"https://api.github.com/users/hwu36/gists{/gist_id}","starred_url":"https://api.github.com/users/hwu36/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hwu36/subscriptions","organizations_url":"https://api.github.com/users/hwu36/orgs","repos_url":"https://api.github.com/users/hwu36/repos","events_url":"https://api.github.com/users/hwu36/events{/privacy}","received_events_url":"https://api.github.com/users/hwu36/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"RE_kwDOBrVDM84N8-LN","tag_name":"v4.1.0","target_commitish":"main","name":"CUTLASS 4.1.0","draft":false,"immutable":false,"prerelease":false,"created_at":"2025-07-22T02:09:56Z","published_at":"2025-07-22T02:14:43Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v4.1.0","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v4.1.0","body":"**CuTe DSL**\r\n* Add aarch64 support, you can now pip install `nvidia-cutlass-dsl` on GB200 systems!\r\n* More examples demonstrating how to use CuTe DSL to write peak-performance kernels\r\n    - [Blackwell Mamba2 SSD](https://github.com/NVIDIA/cutlass/tree/main/examples/python/CuTeDSL/blackwell/mamba2_ssd/mamba2_ssd.py)\r\n    - [Blackwell SM100 persistent dense blockscaled GEMM with static scheduling](https://github.com/NVIDIA/cutlass/tree/main/examples/python/CuTeDSL/blackwell/dense_blockscaled_gemm_persistent.py)\r\n* API updates\r\n    - Please refer to [FUNCTIONALITY.md](https://github.com/NVIDIA/cutlass/blob/main/FUNCTIONALITY.md) for details\r\n\r\n**CUTLASS C++**\r\n* Further enhance Blackwell SM100 Attention kernels in [example 77](https://github.com/NVIDIA/cutlass/tree/main/examples/77_blackwell_fmha/).\r\n    - Add variable sequence length support for FMHA Backward kernel.\r\n    - Add varlen test support to Backward runner.\r\n    - Codes support empty batch sequences.\r\n* Replace `subbyte_iterator` with `cute::recast_ptr` when constructing logical iterators/arrays.\r\n* CuTe changes:\r\n    - Rewrite ArithTuple and ScaledBasis for robustness and clarity.\r\n    - Remove buggy and kludgy `get_layoutA|B|C_MN` and friends from Atoms/TiledX.\r\n    - Factor out `print_latex` and friends and rewrite.\r\n    - Factor out `print_svg` and friends and rewrite.\r\n* Support Blackwell SM100 SIMT packed fp32x2 kernels.\r\n* Support residual add for implicit gemm kernels.\r\n* Various fixes for CUTLASS C++ Python interface's EVT tracer:\r\n    - Add verifier for sm90 to report the invalid input.\r\n    - When adding an edge to the graph, if the edge already exists, add an identity compute node to avoid having multiple parallel edges.\r\n    - Register operations of tanh, sigmoid, exp, gelu to the python ast frontend.\r\n    - Replace the NotImplemented Error by packing all nodes into a single topological visitor node as a fallback.\r\n* Fix profiler bugs in exhaustive perf search.\r\n    - Fix incorrect cluster shape output issue when doing exhaustive search.\r\n    - Fix a bug in profiler grouped GEMM for setting tile scheduler swizzles, cluster shapes, and raster orders.\r\n* Fix some profiler issues.\r\n    - Complete the reference for Blackwell blockwise gemm kernels.\r\n    - Fix incorrect regex logic for L1 test.\r\n","discussion_url":"https://github.com/NVIDIA/cutlass/discussions/2484","reactions":{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/234087117/reactions","total_count":5,"+1":2,"-1":0,"laugh":0,"hooray":1,"confused":0,"heart":0,"rocket":2,"eyes":0}},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/228379646","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/228379646/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/228379646/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v4.0.0","id":228379646,"author":{"login":"kerrmudgeon","id":11259,"node_id":"MDQ6VXNlcjExMjU5","avatar_url":"https://avatars.githubusercontent.com/u/11259?v=4","gravatar_id":"","url":"https://api.github.com/users/kerrmudgeon","html_url":"https://github.com/kerrmudgeon","followers_url":"https://api.github.com/users/kerrmudgeon/followers","following_url":"https://api.github.com/users/kerrmudgeon/following{/other_user}","gists_url":"https://api.github.com/users/kerrmudgeon/gists{/gist_id}","starred_url":"https://api.github.com/users/kerrmudgeon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kerrmudgeon/subscriptions","organizations_url":"https://api.github.com/users/kerrmudgeon/orgs","repos_url":"https://api.github.com/users/kerrmudgeon/repos","events_url":"https://api.github.com/users/kerrmudgeon/events{/privacy}","received_events_url":"https://api.github.com/users/kerrmudgeon/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"RE_kwDOBrVDM84NnMv-","tag_name":"v4.0.0","target_commitish":"main","name":"CUTLASS 4.0.0","draft":false,"immutable":false,"prerelease":false,"created_at":"2025-06-27T13:35:06Z","published_at":"2025-06-27T14:17:40Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v4.0.0","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v4.0.0","body":"**CuTe DSL**\r\n\r\nCuTe DSL is a Python DSL centered around CuTe's abstractions\r\n- Enables authoring kernels in Python to reach peak performance on NVIDIA GPUs\r\n- [Core DSL implementation files](https://github.com/NVIDIA/cutlass/tree/main/python/CuTeDSL)\r\n- [DSL quick start](https://docs.nvidia.com/cutlass/media/docs/pythonDSL/quick_start.html)\r\n- [DSL Overview](https://docs.nvidia.com/cutlass/media/docs/pythonDSL/overview.html)\r\n- [Educational notebooks for getting started with CuTe DSL](https://github.com/NVIDIA/cutlass/tree/main/examples/python/CuTeDSL/notebooks)\r\n\r\n\r\n**CUTLASS C++**\r\n\r\n- Support [Family Specific Architecture Features](https://developer.nvidia.com/blog/nvidia-blackwell-and-nvidia-cuda-12-9-introduce-family-specific-architecture-features/) which was introduced in CUDA 12.9\r\n- Further improved [Blockwise](https://github.com/NVIDIA/cutlass/tree/main/examples/67_hopper_fp8_warp_specialized_gemm_with_blockwise_scaling/67_hopper_fp8_warp_specialized_gemm_with_blockwise_scaling.cu) and [Groupwise](https://github.com/NVIDIA/cutlass/tree/main/examples/67_hopper_fp8_warp_specialized_gemm_with_blockwise_scaling/67_hopper_fp8_warp_specialized_gemm_with_groupwise_scaling.cu) GEMMs on Hopper and Blackwell\r\n- Enhance Blackwell SM100 Attention kernels in [example 77](https://github.com/NVIDIA/cutlass/tree/main/examples/77_blackwell_fmha/)\r\n- Add [Blackwell SM100 implicit GEMM conv fprop/dgrad/wgrad unit tests](https://github.com/NVIDIA/cutlass/tree/main/test/unit/conv/device_3x/)\r\n- New [Hopper SM90 FMHA example](https://github.com/NVIDIA/cutlass/tree/main/examples/88_hopper_fmha/), similar in design to the existing [Blackwell FMHA](https://github.com/NVIDIA/cutlass/tree/main/examples/77_blackwell_fmha/)\r\n- Cute enhancements: [CuTe C++ reduce op](https://github.com/NVIDIA/cutlass/tree/main/include/cute/algorithm/tensor_reduce.hpp) \r\n- Other functional and performance enhancements","reactions":{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/228379646/reactions","total_count":22,"+1":0,"-1":0,"laugh":0,"hooray":10,"confused":0,"heart":1,"rocket":11,"eyes":0}},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/216334427","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/216334427/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/216334427/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v3.9.2","id":216334427,"author":{"login":"hwu36","id":57973641,"node_id":"MDQ6VXNlcjU3OTczNjQx","avatar_url":"https://avatars.githubusercontent.com/u/57973641?v=4","gravatar_id":"","url":"https://api.github.com/users/hwu36","html_url":"https://github.com/hwu36","followers_url":"https://api.github.com/users/hwu36/followers","following_url":"https://api.github.com/users/hwu36/following{/other_user}","gists_url":"https://api.github.com/users/hwu36/gists{/gist_id}","starred_url":"https://api.github.com/users/hwu36/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hwu36/subscriptions","organizations_url":"https://api.github.com/users/hwu36/orgs","repos_url":"https://api.github.com/users/hwu36/repos","events_url":"https://api.github.com/users/hwu36/events{/privacy}","received_events_url":"https://api.github.com/users/hwu36/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"RE_kwDOBrVDM84M5QBb","tag_name":"v3.9.2","target_commitish":"main","name":"CUTLASS 3.9.2","draft":false,"immutable":false,"prerelease":false,"created_at":"2025-05-04T04:00:15Z","published_at":"2025-05-04T04:25:21Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v3.9.2","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v3.9.2","body":"* Fixed [Blockwise](./examples/67_hopper_fp8_warp_specialized_gemm_with_blockwise_scaling/67_hopper_fp8_warp_specialized_gemm_with_blockwise_scaling.cu) and [Groupwise](./examples/67_hopper_fp8_warp_specialized_gemm_with_blockwise_scaling/67_hopper_fp8_warp_specialized_gemm_with_groupwise_scaling.cu) GEMM hang issue when problem size K is 128.\r\n* Optimal code generation with CUDA toolkit versions 12.9.","reactions":{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/216334427/reactions","total_count":5,"+1":3,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":2,"eyes":0}},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/215934244","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/215934244/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/215934244/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v3.9.1","id":215934244,"author":{"login":"hwu36","id":57973641,"node_id":"MDQ6VXNlcjU3OTczNjQx","avatar_url":"https://avatars.githubusercontent.com/u/57973641?v=4","gravatar_id":"","url":"https://api.github.com/users/hwu36","html_url":"https://github.com/hwu36","followers_url":"https://api.github.com/users/hwu36/followers","following_url":"https://api.github.com/users/hwu36/following{/other_user}","gists_url":"https://api.github.com/users/hwu36/gists{/gist_id}","starred_url":"https://api.github.com/users/hwu36/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hwu36/subscriptions","organizations_url":"https://api.github.com/users/hwu36/orgs","repos_url":"https://api.github.com/users/hwu36/repos","events_url":"https://api.github.com/users/hwu36/events{/privacy}","received_events_url":"https://api.github.com/users/hwu36/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"RE_kwDOBrVDM84M3uUk","tag_name":"v3.9.1","target_commitish":"main","name":"CUTLASS 3.9.1","draft":false,"immutable":false,"prerelease":false,"created_at":"2025-05-01T04:27:00Z","published_at":"2025-05-01T04:29:36Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v3.9.1","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v3.9.1","body":"* Fixed Group Gemm hang issue in CUTLASS 3.x\r\n* Improved Hopper [Blockwise](./examples/67_hopper_fp8_warp_specialized_gemm_with_blockwise_scaling/67_hopper_fp8_warp_specialized_gemm_with_blockwise_scaling.cu) and [Groupwise](./examples/67_hopper_fp8_warp_specialized_gemm_with_blockwise_scaling/67_hopper_fp8_warp_specialized_gemm_with_groupwise_scaling.cu) GEMM performance.","reactions":{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/215934244/reactions","total_count":7,"+1":7,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0}},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/214672329","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/214672329/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/214672329/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v3.9.0","id":214672329,"author":{"login":"hwu36","id":57973641,"node_id":"MDQ6VXNlcjU3OTczNjQx","avatar_url":"https://avatars.githubusercontent.com/u/57973641?v=4","gravatar_id":"","url":"https://api.github.com/users/hwu36","html_url":"https://github.com/hwu36","followers_url":"https://api.github.com/users/hwu36/followers","following_url":"https://api.github.com/users/hwu36/following{/other_user}","gists_url":"https://api.github.com/users/hwu36/gists{/gist_id}","starred_url":"https://api.github.com/users/hwu36/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hwu36/subscriptions","organizations_url":"https://api.github.com/users/hwu36/orgs","repos_url":"https://api.github.com/users/hwu36/repos","events_url":"https://api.github.com/users/hwu36/events{/privacy}","received_events_url":"https://api.github.com/users/hwu36/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"RE_kwDOBrVDM84My6PJ","tag_name":"v3.9.0","target_commitish":"main","name":"CUTLASS 3.9.0","draft":false,"immutable":false,"prerelease":false,"created_at":"2025-04-25T01:51:34Z","published_at":"2025-04-25T01:53:42Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v3.9.0","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v3.9.0","body":"* Support for Blackwell SM120 kernels for GeForce GPUs in CUTLASS 3.x API:\r\n  - Collective mainloops that target for:\r\n    * [Blockscaled datatypes with support for dense GEMM](./include/cutlass/gemm/collective/sm120_blockscaled_mma_tma.hpp)\r\n    * [Blockscaled datatypes with support for sparse GEMM](./include/cutlass/gemm/collective/sm120_blockscaled_sparse_mma_tma.hpp)\r\n  - New [GEMM](./include/cutlass/gemm/dispatch_policy.hpp) and [epilogue](./include/cutlass/epilogue/dispatch_policy.hpp) dispatch policies for collectives, kernel layers, and builders.\r\n  - [Blackwell SM120 epilogue](./include/cutlass/epilogue/fusion/sm120_visitor_store_tma_warpspecialized.hpp) and [full set of EVT fusions](./include/cutlass/epilogue/fusion/sm120_callbacks_tma_warpspecialized.hpp).\r\n* Set of examples that demonstrate the usage of the 3.x API for targeting Blackwell SM120 architecture:\r\n  - [Blockscaled GEMM with NVFP4 input datatype and BF16 output tensor](./examples/79_blackwell_geforce_gemm/79a_blackwell_geforce_nvfp4_bf16_gemm.cu).\r\n  - [Blockscaled GEMM with NVFP4 input datatype and NVFP4 output tensor with scale factor generation](./examples/79_blackwell_geforce_gemm/79b_blackwell_geforce_nvfp4_nvfp4_gemm.cu).\r\n  - [Blockscaled GEMM with mixed input datatype (MXFP8 and MXFP6) and BF16 output tensor](./examples/79_blackwell_geforce_gemm/79c_blackwell_geforce_mixed_mxfp8_mxfp6_bf16_gemm.cu).\r\n  - [Grouped GEMM with nvfp4 datatype](./examples/79_blackwell_geforce_gemm/79d_blackwell_geforce_nvfp4_grouped_gemm.cu).\r\n  - [Sparse Blockscaled GEMM with mxfp8 input datatype and BF16 output tensor](./examples/80_blackwell_geforce_sparse_gemm/80a_blackwell_geforce_mxfp8_bf16_sparse_gemm.cu).\r\n  - [Sparse Blockscaled GEMM with NVFP4 input datatype and NVFP4 output tensor](./examples/80_blackwell_geforce_sparse_gemm/80b_blackwell_geforce_nvfp4_nvfp4_sparse_gemm.cu).\r\n* Set of unit tests that demonstrate the usage of both [sparse](./test/unit/gemm/device/sm120_blockscaled_sparse_tensorop_gemm/) and [dense](./test/unit/gemm/device/sm120_blockscaled_tensorop_gemm/) Blackwell SM120 blockscaled GEMM.\r\n* Support for Blackwell SM100 Sparse kernels:\r\n  - Collective mainloop that target for\r\n    * [SM100 Sparse GEMM](./include/cutlass/gemm/collective/sm100_sparse_mma_warpspecialized.hpp)\r\n* Set of example that demonstrate the usage of the 3.x API for targeting Blackwell SM100 Sparse GEMM:\r\n  - [Sparse GEMM](./examples/83_blackwell_sparse_gemm/83_blackwell_sparse_gemm.cu)\r\n  - [Blockscaled Sparse GEMM with NVFP4 input data type](./examples/84_blackwell_narrow_precision_sparse_gemm/84a_blackwell_nvfp4_bf16_sparse_gemm.cu)\r\n  - [Blockscaled Sparse GEMM with mixed input data type (MXFP8 and MXFP4)](./examples/84_blackwell_narrow_precision_sparse_gemm/84b_blackwell_mixed_mxfp8_bf16_sparse_gemm.cu)\r\n* Set of unit tests that demonstrate the usage of [sparse](./test/unit/gemm/device/sm100_sparse_tensorop_gemm) and [blockscaled sparse](./test/unit/gemm/device/sm100_blockscaled_sparse_tensorop_gemm) Blackwell SM100 GEMM.\r\n* A new Multi-head Latent Attention (MLA) for SM100 Blackwell architecture in CUTLASS [example](./examples/77_blackwell_fmha/) covers the flashMLA-like weight-absorbed decoding use-case.\r\n* A new FMHA Backward kernel for SM100 Blackwell architecture extends CUTLASS [example](./examples/77_blackwell_fmha/) to show how the five backward pass MMAs can be fused into a single kernel to achieve high performance.\r\n* A new [distributed GEMM example](./examples/82_blackwell_distributed_gemm/82_blackwell_distributed_gemm.cu) for SM100 Blackwell architecture.\r\n* Enhancement and new support of block-wise and group-wise GEMM for Hopper and Blackwell architectures:\r\n  - Enhancement of [blockwise GEMM](./examples/67_hopper_fp8_warp_specialized_gemm_with_blockwise_scaling/67_hopper_fp8_warp_specialized_gemm_with_blockwise_scaling.cu) for Hopper architecture.\r\n  - Enhancement of [groupwise GEMM](./examples/67_hopper_fp8_warp_specialized_gemm_with_blockwise_scaling/67_hopper_fp8_warp_specialized_gemm_with_groupwise_scaling.cu) for Hopper architecture.\r\n  - Support for [grouped GEMM with blockwise and groupwise scaling](./examples/68_hopper_fp8_warp_specialized_grouped_gemm_with_blockwise_scaling/) for Hopper architecture.\r\n  - Support for [grouped-wise GEMM](./tools/profiler/src/blockwise_gemm_operation_profiler.cu) in CUTLASS profiler.\r\n  - Support for [blockwise GEMM](./examples/81_blackwell_gemm_blockwise/81_blackwell_gemm_blockwise.cu) for Blackwell architecture.\r\n  - Support for [groupwise GEMM](./examples/81_blackwell_gemm_blockwise/81_blackwell_gemm_groupwise.cu) for Blackwell architecture.\r\n  - Support for [grouped GEMM with blockwise](./examples/81_blackwell_gemm_blockwise/81_blackwell_grouped_gemm_blockwise.cu) and [groupwise scaling](./examples/81_blackwell_gemm_blockwise/81_blackwell_grouped_gemm_groupwise.cu) for Blackwell architecture.\r\n* Added support for enhanced kernel performance search (auto-tuning) in CUTLASS profiler:\r\n  - Sorting performance results by GFLOPs/second: Users can now sort the final performance report based on GFLOPs/second, making it easier to identify the most efficient kernels.\r\n  - Exhaustive search for best kernel performance in GFLOPs/second: The profiler now searches for the best-performing kernel across a range of problem sizes, swizzle sizes, rasterization orders, and dynamic cluster configurations to maximize performance.\r\n  - Performance search under a fixed GEMM shape: Enables exhaustive tuning within a fixed GEMM shape, exploring various kernel parameters to find the best configuration.\r\n  - More detailed introductions and examples to leverage this feature can be found in [profiler.md](./media/docs/cpp/profiler.md#exhaustive-search-mode-and-top-k-output-ranking-according-to-performance-in-gflopss).\r\n* Support `void` as the D element in sm100 kernel epilogues.","discussion_url":"https://github.com/NVIDIA/cutlass/discussions/2262","reactions":{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/214672329/reactions","total_count":6,"+1":4,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":2,"eyes":0}},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/201590633","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/201590633/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/201590633/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v3.8.0","id":201590633,"author":{"login":"hwu36","id":57973641,"node_id":"MDQ6VXNlcjU3OTczNjQx","avatar_url":"https://avatars.githubusercontent.com/u/57973641?v=4","gravatar_id":"","url":"https://api.github.com/users/hwu36","html_url":"https://github.com/hwu36","followers_url":"https://api.github.com/users/hwu36/followers","following_url":"https://api.github.com/users/hwu36/following{/other_user}","gists_url":"https://api.github.com/users/hwu36/gists{/gist_id}","starred_url":"https://api.github.com/users/hwu36/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hwu36/subscriptions","organizations_url":"https://api.github.com/users/hwu36/orgs","repos_url":"https://api.github.com/users/hwu36/repos","events_url":"https://api.github.com/users/hwu36/events{/privacy}","received_events_url":"https://api.github.com/users/hwu36/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"RE_kwDOBrVDM84MBAdp","tag_name":"v3.8.0","target_commitish":"main","name":"CUTLASS 3.8.0","draft":false,"immutable":false,"prerelease":false,"created_at":"2025-02-21T05:16:56Z","published_at":"2025-02-21T05:32:15Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v3.8.0","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v3.8.0","body":"\r\nCUTLASS 3.8 is the first release that supports the NVIDIA Blackwell SM100 architecture.\r\nFor a background on Blackwell's new features, please consult the PTX documentation for CUDA 12.8.\r\n\r\n* Support for new CuTe building blocks specifically for Blackwell SM100 architecture:\r\n  - [5th generation Blackwell Tensor Core instructions (TCGen05)](./include/cute/atom/mma_traits_sm100.hpp) via CuTe MMA atoms.\r\n  - Extensions to [Tensor Memory Accelerator](./include/cute/atom/copy_traits_sm100_tma.hpp) via CuTe Copy atoms.\r\n  - Exposure of Blackwell's new tensor memory (note: distinct from TMA) as [`tmem`](./include/cute/pointer.hpp) across CuTe as a first class data locale.\r\n  - Exposure of [`tmem->rmem`, `rmem->tmem` and `smem->tmem data movement instructions`](./include/cute/atom/copy_traits_sm100.hpp) as copy atoms in CuTe.\r\n  - [`make_tmem_copy()`](./include/cute/atom/copy_traits_sm100.hpp) utility method to ease creation of tiled copies for tmem copy atoms.\r\n  - Support for [new variants of LDSM on Blackwell](./include/cute/atom/copy_traits_sm100.hpp) via CuTe Copy atoms.\r\n* Support for new CUTLASS building blocks specifically for Blackwell SM100 architecture:\r\n  - Various narrow precision [FP4, FP6, and FP8](./include/cutlass/exmy_base.h) formats as well as their [block-scaled variants NVFP4, MXFP4, MXFP6, and MXFP8](./include/cutlass/float_subbyte.h)\r\n  - [Pipelines that implement Blackwell specific synchronization](./include/cutlass/pipeline/sm100_pipeline.hpp).\r\n  - [Cluster launch control API supporting preferred and fallback cluster shapes](./include/cutlass/cluster_launch.hpp).\r\n  - Data types including NVFP4, MXFP4, MXFP6, and MXFP8 and all their supported element and scale factor types.\r\n  - Tile schedulers using [Blackwell's Cluster Launch Control (CLC) feature](./media/docs/blackwell_cluster_launch_control.md) to implement dynamic persistence scheduling for [GEMMs](./include/cutlass/gemm/kernel/sm100_tile_scheduler.hpp), and [stream-K](./include/cutlass/gemm/kernel/sm100_tile_scheduler_stream_k.hpp).\r\n  - Extensions to testbeds and reference check code for unit tests and CUTLASS profiler.\r\n* Full support for Blackwell SM100 kernels in CUTLASS 3.x API:\r\n  - [Blackwell specific kernel layers](./include/cutlass/gemm/kernel/sm100_gemm_tma_warpspecialized.hpp) that\r\n    + Implement a new warp-specialization recipe tuned specifically for Blackwell SM100 architecture.\r\n    + Leverage all the new features such as CLC based tile scheduling, preferred cluster, and TMEM based double buffering of accumulators.\r\n    + Support stream-K load balancing for all kernel types everywhere via composable scheduler support.\r\n  - Blackwell collective mainloops that target the TCGen05 MMA instructions (both SS and TS) for\r\n    * [Non-block scaled data types without support for pointer array and grouped GEMM with TMA](./include/cutlass/gemm/collective/sm100_mma_warpspecialized.hpp)\r\n    * [Non-block scaled data types with support for pointer array and grouped GEMM with TMA](./include/cutlass/gemm/collective/sm100_mma_array_warpspecialized.hpp)\r\n    * [Block scaled data types without support for pointer array and grouped GEMM with TMA](./include/cutlass/gemm/collective/sm100_blockscaled_mma_warpspecialized.hpp)\r\n    * [Block scaled data types with support for pointer array and grouped GEMM with TMA](./include/cutlass/gemm/collective/sm100_blockscaled_mma_array_warpspecialized.hpp)\r\n  - Blackwell [collective mainloop for convolution kernels](./include/cutlass/conv/collective/sm100_implicit_gemm_umma_warpspecialized.hpp) supporting non-block scaled data types for fprop, dgrad, and wgrad.\r\n  - New [GEMM](./include/cutlass/gemm/dispatch_policy.hpp), [convolution](./include/cutlass/conv/dispatch_policy.hpp), and [epilogue](./include/cutlass/epilogue/dispatch_policy.hpp) dispatch policies for collectives, kernel layers, and builders.\r\n  - [Blackwell epilogue that supports loading accumulators from `tmem`](./include/cutlass/epilogue/collective/sm100_epilogue_tma_warpspecialized.hpp) and [full set of EVT fusions]().\r\n* CUTLASS library and profiler integration for block scaled data types for kernel emission, profiling, and verification.\r\n  - Support for preferred and fallback cluster shapes via profiler command line arguments parsing to set dynamic cluster shapes.\r\n  - Support for dynamic datatypes by parsing profiler via profiler command line arguments parsing to set dynamic datatype setting in TCGen05 MMA instruction descriptors.\r\n  - Support for mixed input GEMM kernels on Hopper in the profiler.\r\n* New CUTLASS profiler flag `use-cuda-graphs` to reduce overheads when benchmarking launch-bound kernels.\r\n* A new 3.x version of grouped GEMM to the CUTLASS library and generates kernels for Hopper and Blackwell. Now grouped GEMM support is enabled in the CUTLASS profiler (`./cutlass_profiler --operation=GroupedGemm --help` for details).\r\n* Set of examples that demonstrate the usage of the 3.x API for targeting Blackwell SM100 architecture:\r\n  - [Basic FP16 and FP8 GEMMs with minimal changes from Hopper examples](./examples/70_blackwell_gemm/), demonstrating ease of migration for off the shelf kernels using the 3.x collective builder API.\r\n  - GEMM with [opt-in collective builder schedules showcasing available recipes](./examples/71_blackwell_gemm_with_collective_builder/71_blackwell_gemm_with_collective_builder.cu) for Blackwell.\r\n  - Block scaled data type GEMMs targeting Blackwell's native block scaled Tensor Cores:\r\n    + [NVFP4 inputs with BF16 output](./examples/72_blackwell_narrow_precision_gemm/72a_blackwell_nvfp4_bf16_gemm.cu)\r\n    + [NVFP4 inputs with NVFP4 output](./examples/72_blackwell_narrow_precision_gemm/72b_blackwell_nvfp4_nvfp4_gemm.cu)\r\n    + [Mixed MXFP8 and MXFP6 inputs with BF16 output](./examples/72_blackwell_narrow_precision_gemm/72c_blackwell_mixed_mxfp8_bf16_gemm.cu)\r\n  - GEMM example demonstrating [Blackwell's new preferred cluster support via dynamic cluster shapes](./examples/73_blackwell_gemm_preferred_cluster/blackwell_gemm_preferred_cluster.cu) for increased occupancy.\r\n  - [GEMM with CLC based StreamK scheduler for load balancing](./examples/74_blackwell_gemm_streamk/blackwell_gemm_streamk.cu).\r\n  - Grouped GEMM for [vanilla FP8 data inputs](./examples/75_blackwell_grouped_gemm/75_blackwell_grouped_gemm.cu) and [NVFP4 block scaled inputs](./examples/75_blackwell_grouped_gemm/75_blackwell_grouped_gemm_block_scaled.cu).\r\n  - Convolution kernels for [fprop](./examples/76_blackwell_conv/76_blackwell_conv_fprop.cu), [dgrad](./examples/76_blackwell_conv/76_blackwell_conv_dgrad.cu), and [wgrad](./examples/76_blackwell_conv/76_blackwell_conv_wgrad.cu).\r\n  - [Fused multi-head attention fprop kernel](./examples/77_blackwell_fmha/77_blackwell_fmha.cu) supporting fp16/bf16/fp8 data types across head dims of 32,64, and 128.\r\n  - A new BF16x9 GEMM [kernel](./examples/78_blackwell_emulated_bf16x9_gemm/78_blackwell_emulated_bf16x9_gemm.cu) that emulates FP32 GEMM (SGEMM) using BF16 operations.\r\n* Set of examples that demonstrate the usage of the 3.x API for targeting Hopper architecture:\r\n  - A set of new [Hopper grouped GEMM kernels](./examples/69_hopper_mixed_dtype_grouped_gemm/) that support mixed A and B datatypes.\r\n  - A new [Hopper FP8 GEMM with groupwise scaling](./examples/67_hopper_fp8_warp_specialized_gemm_with_blockwise_scaling/67_hopper_fp8_warp_specialized_gemm_with_groupwise_scaling.cu).\r\n* Documentation updates:\r\n  - [Quickstart - instantiating a Blackwell block-scaled GEMM](./media/docs/quickstart.md#instantiating-a-blackwell-gemm-kernel).\r\n  - Detailed [Blackwell block-scaled GEMM functionality documentation](./media/docs/blackwell_functionality.md)\r\n  - A new [functionality documentation](./media/docs/functionality.md) specifically for 3.x API comprehensively documenting all supported kernel types, data types, kernel features, minimum CUDA tookit support etc for 3.x supported architectures.\r\n  - Updates to [compatibility](./README.md#compatibility) section regarding supported compilers, operating systems, CUDA Toolkits, Hardware Architectures, and [Target Architecture](./README.md#Target-Architecture).\r\n\r\nNote: CUTLASS 3.x builds are known to be down on Windows platforms for all CUDA toolkits.\r\nCUTLASS team is working on a fix.","discussion_url":"https://github.com/NVIDIA/cutlass/discussions/2125","reactions":{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/201590633/reactions","total_count":16,"+1":3,"-1":0,"laugh":0,"hooray":6,"confused":0,"heart":2,"rocket":5,"eyes":0}},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/195466137","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/195466137/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/195466137/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v3.7.0","id":195466137,"author":{"login":"hwu36","id":57973641,"node_id":"MDQ6VXNlcjU3OTczNjQx","avatar_url":"https://avatars.githubusercontent.com/u/57973641?v=4","gravatar_id":"","url":"https://api.github.com/users/hwu36","html_url":"https://github.com/hwu36","followers_url":"https://api.github.com/users/hwu36/followers","following_url":"https://api.github.com/users/hwu36/following{/other_user}","gists_url":"https://api.github.com/users/hwu36/gists{/gist_id}","starred_url":"https://api.github.com/users/hwu36/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hwu36/subscriptions","organizations_url":"https://api.github.com/users/hwu36/orgs","repos_url":"https://api.github.com/users/hwu36/repos","events_url":"https://api.github.com/users/hwu36/events{/privacy}","received_events_url":"https://api.github.com/users/hwu36/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"RE_kwDOBrVDM84LppOZ","tag_name":"v3.7.0","target_commitish":"main","name":"CUTLASS 3.7.0","draft":false,"immutable":false,"prerelease":false,"created_at":"2025-01-18T14:53:07Z","published_at":"2025-01-18T15:07:29Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v3.7.0","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v3.7.0","body":"\r\n- A new [Hopper blockwise scaling FP8 GEMM](./examples/67_hopper_fp8_warp_specialized_gemm_with_blockwise_scaling/67_hopper_fp8_warp_specialized_gemm_with_blockwise_scaling.cu) where the operands and block scaling tensor are staged via shared memory.\r\n- [Distributed GEMM](./examples/65_distributed_gemm/65_distributed_gemm.cu) is an experimental pipelined Tensor Parallelism implementation utilizing existing CUTLASS kernels and CUDA runtime features, which can hide the most of communication behind computation.\r\n- Improved persistent grid launch for Hopper kernels with large cluster sizes (>= size of 4) using the new `make_kernel_hardware_info` API as shown in [example 48](./examples/48_hopper_warp_specialized_gemm/48_hopper_warp_specialized_gemm.cu).\r\n- Enabled high precision accumulation for Hopper FP8 Sparse GEMM.","discussion_url":"https://github.com/NVIDIA/cutlass/discussions/2047","reactions":{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/195466137/reactions","total_count":11,"+1":0,"-1":0,"laugh":0,"hooray":11,"confused":0,"heart":0,"rocket":0,"eyes":0}},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/192319781","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/192319781/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/192319781/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v3.6.0","id":192319781,"author":{"login":"hwu36","id":57973641,"node_id":"MDQ6VXNlcjU3OTczNjQx","avatar_url":"https://avatars.githubusercontent.com/u/57973641?v=4","gravatar_id":"","url":"https://api.github.com/users/hwu36","html_url":"https://github.com/hwu36","followers_url":"https://api.github.com/users/hwu36/followers","following_url":"https://api.github.com/users/hwu36/following{/other_user}","gists_url":"https://api.github.com/users/hwu36/gists{/gist_id}","starred_url":"https://api.github.com/users/hwu36/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hwu36/subscriptions","organizations_url":"https://api.github.com/users/hwu36/orgs","repos_url":"https://api.github.com/users/hwu36/repos","events_url":"https://api.github.com/users/hwu36/events{/privacy}","received_events_url":"https://api.github.com/users/hwu36/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"RE_kwDOBrVDM84LdpEl","tag_name":"v3.6.0","target_commitish":"main","name":"CUTLASS 3.6.0","draft":false,"immutable":false,"prerelease":false,"created_at":"2024-12-25T22:11:15Z","published_at":"2024-12-25T22:19:24Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v3.6.0","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v3.6.0","body":"- [Hopper structured sparse GEMM](./examples/62_hopper_sparse_gemm/62_hopper_sparse_gemm.cu).\r\n  + [FP16](./test/unit/gemm/device/sm90_sparse_gemm_f16_f16_f32_tensor_op_f32.cu)\r\n  + [FP8](./test/unit/gemm/device/sm90_sparse_gemm_f8_f8_f32_tensor_op_f32.cu)\r\n  + [INT8](./test/unit/gemm/device/sm90_sparse_gemm_s8_s8_s32_tensor_op_s32.cu)\r\n  + [TF32](./test/unit/gemm/device/sm90_sparse_gemm_tf32_tf32_f32_tensor_op_f32.cu)\r\n- A refactor to the CUTLASS 3.x convolution `kernel::ConvUniversal` [API](./include/cutlass/conv/kernel/sm90_implicit_gemm_tma_warpspecialized.hpp) to bring it in line with `gemm::GemmUniversal`. Now the 3.x convolution API is no longer considered as a beta API.\r\n- [An improved mixed input GEMM](./examples/55_hopper_mixed_dtype_gemm/README.md) and a [lookup table implementation](./examples/55_hopper_mixed_dtype_gemm/55_hopper_int4_fp8_gemm.cu) for `INT4`x`FP8` scale-only mode.\r\n- [EVT nodes for Top-K selection and softmax](./include/cutlass/epilogue/fusion/sm90_visitor_topk_softmax.hpp) and [GEMM example using those](./examples/61_hopper_gemm_with_topk_and_softmax/61_hopper_gemm_with_topk_and_softmax.cu).\r\n- [Programmatic Dependent Launch](./include/cutlass/arch/grid_dependency_control.h) (PDL) that leverages a new Hopper feature to speedup two back-to-back kernels, and its corresponding [documentations](./media/docs/dependent_kernel_launch.md).\r\n- [A new debugging tool, synclog](./include/cutlass/arch/synclog.hpp), for dumping out all synchronization events from within a kernel to a file. Please see [synclog documentation](./media/docs/utilities.md#debugging-asynchronous-kernels-with-cutlasss-built-in-synclog-tool) for details.\r\n- A new TMA-enabled [epilogue](./include/cutlass/epilogue/collective/sm90_epilogue_array_tma_warpspecialized.hpp) for grouped GEMM that brings significant performance improvement, as well as its EVT support.\r\n- A SIMT-enabled pointer-array [epilogue](./include/cutlass/epilogue/collective/sm70_epilogue_vectorized_array.hpp).\r\n- A new [Ping-Pong kernel schedule for Grouped GEMM](./include/cutlass/gemm/kernel/sm90_gemm_array_tma_warpspecialized_pingpong.hpp) and some other optimizations.\r\n- [A new instantiation strategy for CUTLASS profiler kernels](./python/cutlass_library/sm90_shapes.py) along with [improved documentation for instantiation level in CUTLASS profiler](./media/docs/profiler.md#instantiating-more-kernels-with-hopper).\r\n- A new hardware support for comparisons and computations of [`cutlass::bfloat16_t`](./include/cutlass/bfloat16.h)\r\n- Fixed use of isnan on Windows for [`half_t`](./test/unit/core/functional.cu).","discussion_url":"https://github.com/NVIDIA/cutlass/discussions/2013","reactions":{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/192319781/reactions","total_count":3,"+1":3,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0}},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/172644715","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/172644715/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/172644715/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v3.5.1","id":172644715,"author":{"login":"hwu36","id":57973641,"node_id":"MDQ6VXNlcjU3OTczNjQx","avatar_url":"https://avatars.githubusercontent.com/u/57973641?v=4","gravatar_id":"","url":"https://api.github.com/users/hwu36","html_url":"https://github.com/hwu36","followers_url":"https://api.github.com/users/hwu36/followers","following_url":"https://api.github.com/users/hwu36/following{/other_user}","gists_url":"https://api.github.com/users/hwu36/gists{/gist_id}","starred_url":"https://api.github.com/users/hwu36/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hwu36/subscriptions","organizations_url":"https://api.github.com/users/hwu36/orgs","repos_url":"https://api.github.com/users/hwu36/repos","events_url":"https://api.github.com/users/hwu36/events{/privacy}","received_events_url":"https://api.github.com/users/hwu36/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"RE_kwDOBrVDM84KSllr","tag_name":"v3.5.1","target_commitish":"main","name":"CUTLASS 3.5.1","draft":false,"immutable":false,"prerelease":false,"created_at":"2024-08-20T02:21:42Z","published_at":"2024-08-29T20:15:44Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v3.5.1","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v3.5.1","body":"- [Minimal SM90 WGMMA + TMA GEMM example in 100 lines of code](./examples/cute/tutorial/wgmma_sm90.cu).\r\n- [Exposure of L2 `cache_hint`s in TMA copy atoms](./include/cute/arch/copy_sm90_tma.hpp#L48)\r\n- Exposure of raster order and tile swizzle extent in [CUTLASS library profiler](./media/docs/profiler.md#GEMM), and\r\n[example 48](./examples/48_hopper_warp_specialized_gemm/48_hopper_warp_specialized_gemm.cu).\r\n- [TMA store based and EVT supported epilogues](./include/cutlass/epilogue/collective/sm90_epilogue_array_tma_warpspecialized.hpp) for [Hopper pointer array batched kernels](./test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_ptr_array.cu).\r\n- A new [`GemmSparseUniversal` API for CUTLASS 2.x Ampere kernels](./include/cutlass/gemm/device/gemm_sparse_universal.h) to enable serial and parallel split-k for sparse tensor cores and new tiny tile sizes to better support LLM inference.\r\n- [CUDA host adapter](./include/cutlass/cuda_host_adapter.hpp) extensions to support TMA descriptor construction driver APIs.\r\n- Inclusion of more [Hopper fprop, dgrad, and wgrad convolution kernels in CUTLASS library and profiler](./python/cutlass_library/generator.py).\r\n- Support for residual add (beta != 0) in convolution kernels.\r\n- A new convolution [epilogue](./examples/16_ampere_tensorop_conv2dfprop/ampere_tensorop_conv2dfprop.cu#L269) for CUTLASS 2.x to support non-packed NHWC output.\r\n- A refactor of [include files throughout CUTLASS core directories](./include/cutlass/gemm/collective/collective_mma_decl.hpp) to reduce circular dependencies and [tests to guard against them](./test/self_contained_includes/CMakeLists.txt).\r\n- [A guide for setting up VSCode to work well with CUTLASS](./media/docs/ide_setup.md) and [expanded code style guide](./media/docs/programming_guidelines.md).\r\n- Better support for MSVC as a host compiler.\r\n- Many performance optimizations, improvements, and bug fixes including fixes for FlashAttention-2.\r\n- Optimal code generation with CUDA toolkit versions 12.4 and 12.5u1.\r\n- NOTICE:\r\n  + Upcoming CUTLASS 3.6 release will include a breaking refactor to the CUTLASS 3.x convolution `kernel::ConvUniversal` API to bring it in line with `gemm::GemmUniversal`. After this, the 3.x convolution API will no longer be considered as a beta API.\r\n  + Upcoming CUTLASS 3.6 release will include a breaking refactor to the Hopper TMA pointer array batched epilogue in order to support grouped GEMMs.\r\n","discussion_url":"https://github.com/NVIDIA/cutlass/discussions/1759","reactions":{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/172644715/reactions","total_count":19,"+1":1,"-1":0,"laugh":0,"hooray":12,"confused":0,"heart":0,"rocket":6,"eyes":0}},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/150858802","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/150858802/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/150858802/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v3.5.0","id":150858802,"author":{"login":"hwu36","id":57973641,"node_id":"MDQ6VXNlcjU3OTczNjQx","avatar_url":"https://avatars.githubusercontent.com/u/57973641?v=4","gravatar_id":"","url":"https://api.github.com/users/hwu36","html_url":"https://github.com/hwu36","followers_url":"https://api.github.com/users/hwu36/followers","following_url":"https://api.github.com/users/hwu36/following{/other_user}","gists_url":"https://api.github.com/users/hwu36/gists{/gist_id}","starred_url":"https://api.github.com/users/hwu36/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hwu36/subscriptions","organizations_url":"https://api.github.com/users/hwu36/orgs","repos_url":"https://api.github.com/users/hwu36/repos","events_url":"https://api.github.com/users/hwu36/events{/privacy}","received_events_url":"https://api.github.com/users/hwu36/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"RE_kwDOBrVDM84I_ewy","tag_name":"v3.5.0","target_commitish":"main","name":"CUTLASS 3.5.0","draft":false,"immutable":false,"prerelease":false,"created_at":"2024-04-12T01:33:40Z","published_at":"2024-04-12T01:40:40Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v3.5.0","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v3.5.0","body":"- Implicit GEMM Convolutions targeting Hopper SM90A via WGMMA + [TMA im2col](./include/cute/atom/copy_traits_sm90_im2col.hpp).\r\n  + Native implementation in CUTLASS 3.x using CuTe, mirroring the [same design hierarchy as that of GEMMs](./media/docs/gemm_api_3x.md).\r\n  + Support for 1D, 2D, and 3D convolutions in a [rank-agnostic fashion](./include/cutlass/conv/convnd_problem_shape.hpp).\r\n  + Support for [Fprop](./test/unit/conv/device_3x/fprop/sm90_conv3d_fprop_implicit_gemm_s8_s8_s32_tensorop_s32.cu), [Dgrad](./test/unit/conv/device_3x/dgrad/sm90_conv2d_dgrad_implicit_gemm_f16_f16_f32_tensorop_f16.cu), and [Wgrad](./test/unit/conv/device_3x/wgrad/sm90_conv1d_wgrad_implicit_gemm_f16_f16_f32_tensorop_f16.cu) algorithms.\r\n  + [CUTLASS profiler support](./python/cutlass_library/conv3x_emitter.py) for 2D and 3D convolutions implemented via the 3.x API.\r\n  + NOTE: this is a beta release. Further updates to CUTLASS will include major performance improvements, feature enablement, and possible breaking changes to the API until 3.7 release. Your feedback is welcome on the design!\r\n- Support for [Ada (SM89) FP8 tensor cores via the 2.x API](./examples/58_ada_fp8_gemm/ada_fp8_gemm.cu). Requires CUDA 12.4 or newer.\r\n- [Ampere gather/scatter convolution example](./examples/59_ampere_gather_scatter_gemm/README.md) in CuTe and CUTLASS 3.x.\r\n  + Showcasing how custom kernels can be written and optimized using CUTLASS 3.x and CuTe and the general strategy for implementing convolutions as specializations of GETTs.\r\n  + Implementation of a coarse grained sparse gather/scatter kernel achieving peak performance on Ampere class tensor cores.\r\n- 32x and 16x tile sizes are added to CUTLASS 2.x to improve the performance of narrow-tall and wide-short matrices.\r\n- Updates to CuTe documentation for [`cute::Tensor<>`](./media/docs/cute/03_tensor.md), [MMA atoms](./media/docs/cute/0t_mma_atom.md), and an overhauled [CuTe GEMM tutorial series](./examples/cute/tutorial).\r\n- Extensions to CuTe to support [L2 prefetching](./include/cute/algorithm/prefetch.hpp) and [TMA store+reductions](./include/cute/arch/copy_sm90_tma.hpp#L1337).\r\n- Remove C++11 requirement on a few CUTLASS 2.x API header files. All CUTLASS files now require C++17.\r\n- Fixes to greatly reduce build warnings.\r\n- Updates and bugfixes from the community (thanks!)","discussion_url":"https://github.com/NVIDIA/cutlass/discussions/1475","reactions":{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/150858802/reactions","total_count":10,"+1":0,"-1":0,"laugh":0,"hooray":6,"confused":0,"heart":0,"rocket":1,"eyes":3}},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/142233843","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/142233843/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/142233843/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v3.4.1","id":142233843,"author":{"login":"hwu36","id":57973641,"node_id":"MDQ6VXNlcjU3OTczNjQx","avatar_url":"https://avatars.githubusercontent.com/u/57973641?v=4","gravatar_id":"","url":"https://api.github.com/users/hwu36","html_url":"https://github.com/hwu36","followers_url":"https://api.github.com/users/hwu36/followers","following_url":"https://api.github.com/users/hwu36/following{/other_user}","gists_url":"https://api.github.com/users/hwu36/gists{/gist_id}","starred_url":"https://api.github.com/users/hwu36/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hwu36/subscriptions","organizations_url":"https://api.github.com/users/hwu36/orgs","repos_url":"https://api.github.com/users/hwu36/repos","events_url":"https://api.github.com/users/hwu36/events{/privacy}","received_events_url":"https://api.github.com/users/hwu36/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"RE_kwDOBrVDM84IelDz","tag_name":"v3.4.1","target_commitish":"main","name":"CUTLASS 3.4.1","draft":false,"immutable":false,"prerelease":false,"created_at":"2024-02-15T20:48:34Z","published_at":"2024-02-15T21:03:37Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v3.4.1","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v3.4.1","body":"- Statically available [CUTLASS Version macros](/include/cutlass/version.h) that allow for handling API changes between CUTLASS releases on the users' side.\r\n- Improvements for Hopper [Group-GEMMs](/examples/57_hopper_grouped_gemm) and [Pointer-Array Batched GEMMs](/examples/56_hopper_ptr_array_batched_gemm).\r\n- Updates and bugfixes from the community (thanks!).","discussion_url":"https://github.com/NVIDIA/cutlass/discussions/1349","reactions":{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/142233843/reactions","total_count":7,"+1":0,"-1":0,"laugh":0,"hooray":7,"confused":0,"heart":0,"rocket":0,"eyes":0}},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/137303762","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/137303762/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/137303762/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v3.4.0","id":137303762,"author":{"login":"hwu36","id":57973641,"node_id":"MDQ6VXNlcjU3OTczNjQx","avatar_url":"https://avatars.githubusercontent.com/u/57973641?v=4","gravatar_id":"","url":"https://api.github.com/users/hwu36","html_url":"https://github.com/hwu36","followers_url":"https://api.github.com/users/hwu36/followers","following_url":"https://api.github.com/users/hwu36/following{/other_user}","gists_url":"https://api.github.com/users/hwu36/gists{/gist_id}","starred_url":"https://api.github.com/users/hwu36/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hwu36/subscriptions","organizations_url":"https://api.github.com/users/hwu36/orgs","repos_url":"https://api.github.com/users/hwu36/repos","events_url":"https://api.github.com/users/hwu36/events{/privacy}","received_events_url":"https://api.github.com/users/hwu36/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"RE_kwDOBrVDM84ILxbS","tag_name":"v3.4.0","target_commitish":"main","name":"CUTLASS 3.4.0","draft":false,"immutable":false,"prerelease":false,"created_at":"2024-01-16T19:37:22Z","published_at":"2024-01-16T22:39:37Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v3.4.0","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v3.4.0","body":"- Improved [Mixed-input Hopper GEMMs](https://github.com/NVIDIA/cutlass/blob/main/examples/55_hopper_mixed_dtype_gemm) supporting {16-bit, 8-bit} x {8-bit, 4-bit} input types with fast numerical converters and group scaling factors tuned for optimal performance on Hopper H100.\r\n- Beta release of [Pointer-Array Batched GEMMs](https://github.com/NVIDIA/cutlass/blob/main/examples/56_hopper_ptr_array_batched_gemm) utilizing TMA and Hopper H100 tensor cores now available. (Requires CUDA 12.3 or above)\r\n- Beta release of [Group-GEMM](https://github.com/NVIDIA/cutlass/blob/main/examples/57_hopper_grouped_gemm) - commonly used in optimization of Mixture-Of-Expert models, is now available on Hopper GPUs taking advantage of TMA and Hopper H100 tensor cores. (Requires CUDA 12.3 or above)\r\n- [Ampere Sparse GEMM](https://github.com/NVIDIA/cutlass/blob/main/examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm_with_visitor.cu) supports Epilogue Visitor Tree (EVT) now.\r\n- Impovements to NamedBarriers including details of [ReservedNamedBarriers](https://github.com/NVIDIA/cutlass/blob/main/include/cutlass/arch/barrier.h) used within the CUTLASS library.\r\n- Improved [CuTe documentation](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute) including improved clarity and depth of [Quickstart](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/00_quickstart.md), [CuTe Layout](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/01_layout.md), and [CuTe Layout Algebra](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/02_layout_algebra.md). Associated code comments, post-conditions, and details in [CuTe Core Unit Tests](https://github.com/NVIDIA/cutlass/blob/main/test/unit/cute/core) also improved.","discussion_url":"https://github.com/NVIDIA/cutlass/discussions/1307","reactions":{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/137303762/reactions","total_count":13,"+1":2,"-1":0,"laugh":0,"hooray":6,"confused":0,"heart":0,"rocket":5,"eyes":0}},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/132768431","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/132768431/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/132768431/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v3.3.0","id":132768431,"author":{"login":"hwu36","id":57973641,"node_id":"MDQ6VXNlcjU3OTczNjQx","avatar_url":"https://avatars.githubusercontent.com/u/57973641?v=4","gravatar_id":"","url":"https://api.github.com/users/hwu36","html_url":"https://github.com/hwu36","followers_url":"https://api.github.com/users/hwu36/followers","following_url":"https://api.github.com/users/hwu36/following{/other_user}","gists_url":"https://api.github.com/users/hwu36/gists{/gist_id}","starred_url":"https://api.github.com/users/hwu36/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hwu36/subscriptions","organizations_url":"https://api.github.com/users/hwu36/orgs","repos_url":"https://api.github.com/users/hwu36/repos","events_url":"https://api.github.com/users/hwu36/events{/privacy}","received_events_url":"https://api.github.com/users/hwu36/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"RE_kwDOBrVDM84H6eKv","tag_name":"v3.3.0","target_commitish":"main","name":"CUTLASS 3.3.0","draft":false,"immutable":false,"prerelease":false,"created_at":"2023-12-05T20:35:41Z","published_at":"2023-12-06T01:55:57Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v3.3.0","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v3.3.0","body":"- New [Mixed-input Hopper GEMMs](/examples/55_hopper_mixed_dtype_gemm) support covering 16-bit x 8-bit input types with optimal performance.\r\n- New [Mixed-input Ampere GEMMs](https://github.com/NVIDIA/cutlass/pull/1084) with support for canonical layouts (TN). The implementation supports upcast on operandB {fp16, bf16} x {s8, u8} and upcast on operandA {s8, u8} x {fp16, bf16}. They also include fast numeric conversion recipes and warp level shuffles to achieve optimal performance.\r\n- New [Copy Async based Hopper GEMMs](/test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_alignx_tensor_op_f32_warpspecialized_cooperative.cu) - which support lower than 16B aligned input tensors (across s8/fp8/fp16/bf16/tf32 types) with optimal performance. As a part of this, new kernel schedules, and Copy Ops [SM80\\_CP\\_ASYNC\\_CACHE\\_\\*](/include/cute/arch/copy_sm80.hpp) were also added.\r\n- EVT Support for RELU with Aux bitmap tensor store (used in dRELU). See [SM90 EVT fusions](/include/cutlass/epilogue/fusion/sm90_visitor_compute_tma_warpspecialized.hpp) for details.\r\n- Various subbyte enhancements like tagged device ptrs, support for vectorized copy, various operators to treat subbyte iterators as pointers, and full-fledged CuTe Tensor support.\r\n- Support for Clang as a host compiler. \r\n- Support for void-C kernels and SM80 mixed-input GEMMs in the CUTLASS Python interface","discussion_url":"https://github.com/NVIDIA/cutlass/discussions/1241","reactions":{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/132768431/reactions","total_count":11,"+1":0,"-1":0,"laugh":0,"hooray":7,"confused":0,"heart":0,"rocket":4,"eyes":0}},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/126829483","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/126829483/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/126829483/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v3.2.2","id":126829483,"author":{"login":"hwu36","id":57973641,"node_id":"MDQ6VXNlcjU3OTczNjQx","avatar_url":"https://avatars.githubusercontent.com/u/57973641?v=4","gravatar_id":"","url":"https://api.github.com/users/hwu36","html_url":"https://github.com/hwu36","followers_url":"https://api.github.com/users/hwu36/followers","following_url":"https://api.github.com/users/hwu36/following{/other_user}","gists_url":"https://api.github.com/users/hwu36/gists{/gist_id}","starred_url":"https://api.github.com/users/hwu36/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hwu36/subscriptions","organizations_url":"https://api.github.com/users/hwu36/orgs","repos_url":"https://api.github.com/users/hwu36/repos","events_url":"https://api.github.com/users/hwu36/events{/privacy}","received_events_url":"https://api.github.com/users/hwu36/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"RE_kwDOBrVDM84Hj0Or","tag_name":"v3.2.2","target_commitish":"release/3.2.x","name":"CUTLASS 3.2.2","draft":false,"immutable":false,"prerelease":false,"created_at":"2023-10-26T18:07:30Z","published_at":"2023-10-26T18:17:14Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v3.2.2","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v3.2.2","body":"Bug fix for illegal memory access issue hit by Flash Attention tests in PyTorch. See #1138 for details.","discussion_url":"https://github.com/NVIDIA/cutlass/discussions/1162","reactions":{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/126829483/reactions","total_count":4,"+1":1,"-1":0,"laugh":2,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":1}},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/122693193","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/122693193/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/122693193/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v3.2.1","id":122693193,"author":{"login":"hwu36","id":57973641,"node_id":"MDQ6VXNlcjU3OTczNjQx","avatar_url":"https://avatars.githubusercontent.com/u/57973641?v=4","gravatar_id":"","url":"https://api.github.com/users/hwu36","html_url":"https://github.com/hwu36","followers_url":"https://api.github.com/users/hwu36/followers","following_url":"https://api.github.com/users/hwu36/following{/other_user}","gists_url":"https://api.github.com/users/hwu36/gists{/gist_id}","starred_url":"https://api.github.com/users/hwu36/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hwu36/subscriptions","organizations_url":"https://api.github.com/users/hwu36/orgs","repos_url":"https://api.github.com/users/hwu36/repos","events_url":"https://api.github.com/users/hwu36/events{/privacy}","received_events_url":"https://api.github.com/users/hwu36/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"RE_kwDOBrVDM84HUCZJ","tag_name":"v3.2.1","target_commitish":"main","name":"CUTLASS 3.2.1","draft":false,"immutable":false,"prerelease":false,"created_at":"2023-09-26T21:28:00Z","published_at":"2023-09-26T21:47:03Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v3.2.1","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v3.2.1","body":"- Python support SM90 Epilogue Visitor Tree (EVT) on top of the C++ support released in 3.2.0.\r\n- SM80 EVT support in C++ and Python.\r\n- Other SM90 epilogue improvements.\r\n- Splitting CUTLASS library into smaller units based on operation, arch and datatypes. See https://github.com/NVIDIA/cutlass/discussions/1105 for details.\r\n- Making tools/library/scripts packageable - tools/library/scripts is now moving to python/cutlass_library. See the Python [README](https://github.com/NVIDIA/cutlass/blob/main/python/README.md) for details.\r\n- SM90 TF32 kernel improvements for all layouts.\r\n- SM90 rasterization direction support in the CUTLASS profiler.\r\n- Improvement for CUTLASS profiler build times.\r\n- Remove Python-C++ bindings.","discussion_url":"https://github.com/NVIDIA/cutlass/discussions/1114","reactions":{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/122693193/reactions","total_count":2,"+1":0,"-1":0,"laugh":0,"hooray":2,"confused":0,"heart":0,"rocket":0,"eyes":0}},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/118860115","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/118860115/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/118860115/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v3.2.0","id":118860115,"author":{"login":"hwu36","id":57973641,"node_id":"MDQ6VXNlcjU3OTczNjQx","avatar_url":"https://avatars.githubusercontent.com/u/57973641?v=4","gravatar_id":"","url":"https://api.github.com/users/hwu36","html_url":"https://github.com/hwu36","followers_url":"https://api.github.com/users/hwu36/followers","following_url":"https://api.github.com/users/hwu36/following{/other_user}","gists_url":"https://api.github.com/users/hwu36/gists{/gist_id}","starred_url":"https://api.github.com/users/hwu36/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hwu36/subscriptions","organizations_url":"https://api.github.com/users/hwu36/orgs","repos_url":"https://api.github.com/users/hwu36/repos","events_url":"https://api.github.com/users/hwu36/events{/privacy}","received_events_url":"https://api.github.com/users/hwu36/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"RE_kwDOBrVDM84HFalT","tag_name":"v3.2.0","target_commitish":"main","name":"CUTLASS 3.2","draft":false,"immutable":false,"prerelease":false,"created_at":"2023-08-28T00:41:57Z","published_at":"2023-08-28T00:50:51Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v3.2.0","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v3.2.0","body":"- New warp-specialized persistent FP8 GEMM kernel [kernel schedules](https://github.com/NVIDIA/cutlass/blob/main/include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized_cooperative.hpp) and [mainloops](https://github.com/NVIDIA/cutlass/blob/main/include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized_fp8.hpp) targeting Hopper architecture that achieve great performance with TMA, WGMMA, and threadblock clusters. An example showcasing [Hopper warp-specialized FP8 GEMMs](https://github.com/NVIDIA/cutlass/blob/main/examples/54_hopper_fp8_warp_specialized_gemm).\r\n- New [Epilogue Visitor Tree (EVT)](https://github.com/NVIDIA/cutlass/blob/main/examples/49_hopper_gemm_with_collective_builder/49_collective_builder.cu) support for Hopper TMA epilogues. EVTs allows for user-defined customized epilogue fusion patterns without having to write a new epilogue.\r\n- [Stream-K](https://github.com/NVIDIA/cutlass/blob/main/include/cutlass/gemm/kernel/sm90_tile_scheduler_stream_k.hpp) feature for Hopper. Note that this is only a functional implementation of stream-K, and should not be used for performance comparison. Optimizations are expected in a future release.\r\n- Improved CTA rasterization and support for CTA swizzling for Hopper kernels using the [Tile Scheduler](https://github.com/NVIDIA/cutlass/blob/main/include/cutlass/gemm/kernel/sm90_tile_scheduler.hpp).\r\n- Improved performance for [warp-specialized TensorFloat-32 (TF32) GEMM kernels](https://github.com/NVIDIA/cutlass/blob/main/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_tensor_op_f32_gmma_rs_cluster_warpspecialized.cu) targeting Hopper TMA.\r\n- [Hopper GEMM+Permute](https://github.com/NVIDIA/cutlass/blob/main/examples/53_hopper_gemm_permute/53_hopper_gemm_permute.cu), an example of fusing tensor reordering (permutation) with GEMM mainloop or epilogue.\r\n-  New CUTLASS 2D Convolution Python interface. New [example](https://github.com/NVIDIA/cutlass/blob/main/examples/python/03_basic_conv2d.ipynb) here.\r\n- Support for Windows (MSVC) builds.","discussion_url":"https://github.com/NVIDIA/cutlass/discussions/1069","reactions":{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/118860115/reactions","total_count":7,"+1":5,"-1":0,"laugh":0,"hooray":2,"confused":0,"heart":0,"rocket":0,"eyes":0}},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/104169383","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/104169383/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/104169383/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v3.1.0","id":104169383,"author":{"login":"hwu36","id":57973641,"node_id":"MDQ6VXNlcjU3OTczNjQx","avatar_url":"https://avatars.githubusercontent.com/u/57973641?v=4","gravatar_id":"","url":"https://api.github.com/users/hwu36","html_url":"https://github.com/hwu36","followers_url":"https://api.github.com/users/hwu36/followers","following_url":"https://api.github.com/users/hwu36/following{/other_user}","gists_url":"https://api.github.com/users/hwu36/gists{/gist_id}","starred_url":"https://api.github.com/users/hwu36/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hwu36/subscriptions","organizations_url":"https://api.github.com/users/hwu36/orgs","repos_url":"https://api.github.com/users/hwu36/repos","events_url":"https://api.github.com/users/hwu36/events{/privacy}","received_events_url":"https://api.github.com/users/hwu36/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"RE_kwDOBrVDM84GNX-n","tag_name":"v3.1.0","target_commitish":"main","name":"CUTLASS 3.1","draft":false,"immutable":false,"prerelease":false,"created_at":"2023-05-24T16:40:31Z","published_at":"2023-05-24T20:10:41Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v3.1.0","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v3.1.0","body":"* New CUTLASS Python interface that aims to provide an ease-of-use interface for instantiating, emitting, compiling, and running CUTLASS kernels via Python. More details [here](/python/README.md) and new [examples](/examples/python).\r\n* New [efficient epilogues](test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_cooperative.cu#L783) using TMA for Hopper.\r\n* Support for [fused epilogues](test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_cooperative_bias_elementwise.cu), such Bias, ReLU and GELU, using the new efficient epilogues.\r\n* New [warp-specialized TensorFloat-32 (TF32) GEMM kernels](test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_tensor_op_f32_gmma_rs_cluster_warpspecialized.cu) targeting Hopper TMA.\r\n* New [*warp-specialized persistent cooperative*](include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized_cooperative.hpp) kernel design that allows for larger tile sizes and improves performance on Hopper.\r\n* An [example](examples/51_hopper_gett) showcasing GEMM-Like Tensor-Tensor Contraction (GETT) capability on Hopper.\r\n* Epilogue builders. Similar to mainloop builders (see [example 49](/examples/49_hopper_gemm_with_collective_builder/49_collective_builder.cu)), epilogue builders aim to generate the best-possible epilogue while exposing incremental opt-ins for greater customization.\r\n* Profiler support for overriding kernel and epilogue builder auto schedules for 3.x API kernels, allowing specific policies to be run in the CUTLASS profiler.\r\n* Performance optimizations for the [*warp-specialized persistent ping-pong*](include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized_pingpong.hpp) kernel.\r\n* Changes to the [GEMM API 3.x](media/docs/gemm_api_3x.md), involving the host-facing arguments and the underlying `Params` structs.\r\n* [FMHA Backward Pass](examples/41_fused_multi_head_attention/fused_multi_head_attention_backward.cu) from Meta xFormers.\r\n* [Streamk GEMM with Broadcast](examples/47_ampere_gemm_universal_streamk/ampere_gemm_universal_streamk_broadcast.cu) enables epilogue broadcast with StreamK GEMM.\r\n* [Batched B2B GEMM](examples/13_two_tensor_op_fusion) now can run multiple Back-to-Back GEMM with the same problem size in parallel.\r\n* [Batched Strided GEMV](test/unit/gemm/device/gemv.cu) support both row major and column major input matrix.\r\n* [Permute + GEMM fusion](examples/39_gemm_permute) can fuse Permute with following GEMM now.  Before, we only support fusing GEMM with Permute in the epilogue.\r\n* [Row Broadcast](include/cutlass/epilogue/threadblock/predicated_tile_iterator_row_broadcast.h) can be fused in the epilogue.\r\n* The GitHub branch is renamed from `master` to `main` in this release.\r\n* Optimal performance using [**CUDA 12.1**](https://developer.nvidia.com/cuda-downloads)\r\n* Updates and bugfixes from the community (thanks!)\r\n","discussion_url":"https://github.com/NVIDIA/cutlass/discussions/960","reactions":{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/104169383/reactions","total_count":17,"+1":0,"-1":0,"laugh":0,"hooray":10,"confused":0,"heart":3,"rocket":4,"eyes":0}},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/95161886","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/95161886/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/95161886/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v3.0.0","id":95161886,"author":{"login":"hwu36","id":57973641,"node_id":"MDQ6VXNlcjU3OTczNjQx","avatar_url":"https://avatars.githubusercontent.com/u/57973641?v=4","gravatar_id":"","url":"https://api.github.com/users/hwu36","html_url":"https://github.com/hwu36","followers_url":"https://api.github.com/users/hwu36/followers","following_url":"https://api.github.com/users/hwu36/following{/other_user}","gists_url":"https://api.github.com/users/hwu36/gists{/gist_id}","starred_url":"https://api.github.com/users/hwu36/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hwu36/subscriptions","organizations_url":"https://api.github.com/users/hwu36/orgs","repos_url":"https://api.github.com/users/hwu36/repos","events_url":"https://api.github.com/users/hwu36/events{/privacy}","received_events_url":"https://api.github.com/users/hwu36/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"RE_kwDOBrVDM84FrA4e","tag_name":"v3.0.0","target_commitish":"master","name":"CUTLASS 3.0","draft":false,"immutable":false,"prerelease":false,"created_at":"2023-03-09T20:27:40Z","published_at":"2023-03-10T04:19:59Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v3.0.0","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v3.0.0","body":"## [3.0.0](https://github.com/NVIDIA/cutlass/releases/tag/v3.0.0) (2023-01-23)\r\n* [CuTe](/media/docs/cute/00_quickstart.md), a [new core library and backend](/include/cute) for CUTLASS 3.0 that defines a single Layout vocabulary type and an associated algebra of layouts for a much more expressive and composable abstraction for tensors, sets of parallel agents, and operations by said agents on tensors.\r\n* [A new conceptual operation hierarchy](media/docs/cutlass_3x_design.md) that replaces the architecture-centric hierarchy of CUTLASS 2.x and [documentation for CUTLASS 3.0's GEMM API changes](/media/docs/gemm_api_3x.md).\r\n* Strict API backwards compatibility that exposes both 2.x and 3.x API kernels through the same [`device::GemmUniversalAdapter`](include/cutlass/gemm/device/gemm_universal_adapter.h) and [`kernel::GemmUniversal`](include/cutlass/gemm/kernel/gemm_universal.hpp) types, allowing users to include both APIs in the same translation units. More information can be found in the [3.x backwards compatibility section](media/docs/cutlass_3x_backwards_compatibility.md).\r\n* Updates to [Functionality](media/docs/functionality.md) which directs users on which kernels are supported via CUTLASS-2 and CUTLASS-3.\r\n* Updates to [Compatibility](/README.md#compatibility) Section regarding supported compilers, operating systems, CUDA Toolkits, Hardware Architectures and [Target Architecture](/README.md#Target-Architecture).\r\n* New warp-specialized GEMM [kernel schedules](include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized.hpp) and [mainloops](include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized.hpp) targeting Hopper architecture that achieve great performance with TMA, WGMMA, and threadblock clusters.\r\n* Extensions to CUTLASS profiler to support threadblock cluster shapes in library and profiler tile configurations.\r\n* [CUTLASS library integration](/tools/library/src/gemm_operation_3x.hpp) for 3.x API kernels built through the new `CollectiveBuilder` API, enabling CUTLASS profiler.\r\n* Support for [Hopper GEMMs](examples/48_hopper_warp_specialized_gemm) through the new 3.0 API with CuTe-based exposure of the Hopper [Tensor Memory Accelerator](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor) and [WGMMA Tensor Core](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-instructions) features.\r\n* Set of examples that demonstrate the usage of the new 3.0 API to easily build GEMM kernels targeting Hopper: examples [48](examples/48_hopper_warp_specialized_gemm), [49](examples/49_hopper_gemm_schedules_with_collective_builder), and [50](examples/50_hopper_gemm_with_epilogue_swizzle).","reactions":{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/95161886/reactions","total_count":28,"+1":16,"-1":0,"laugh":1,"hooray":11,"confused":0,"heart":0,"rocket":0,"eyes":0}},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/89778027","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/89778027/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/89778027/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v2.11.0","id":89778027,"author":{"login":"kerrmudgeon","id":11259,"node_id":"MDQ6VXNlcjExMjU5","avatar_url":"https://avatars.githubusercontent.com/u/11259?v=4","gravatar_id":"","url":"https://api.github.com/users/kerrmudgeon","html_url":"https://github.com/kerrmudgeon","followers_url":"https://api.github.com/users/kerrmudgeon/followers","following_url":"https://api.github.com/users/kerrmudgeon/following{/other_user}","gists_url":"https://api.github.com/users/kerrmudgeon/gists{/gist_id}","starred_url":"https://api.github.com/users/kerrmudgeon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kerrmudgeon/subscriptions","organizations_url":"https://api.github.com/users/kerrmudgeon/orgs","repos_url":"https://api.github.com/users/kerrmudgeon/repos","events_url":"https://api.github.com/users/kerrmudgeon/events{/privacy}","received_events_url":"https://api.github.com/users/kerrmudgeon/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"RE_kwDOBrVDM84FWedr","tag_name":"v2.11.0","target_commitish":"master","name":"CUTLASS 2.11","draft":false,"immutable":false,"prerelease":false,"created_at":"2023-01-20T21:32:57Z","published_at":"2023-01-20T21:35:27Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v2.11.0","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v2.11.0","body":"## [2.11.0](https://github.com/NVIDIA/cutlass/releases/tag/v2.11.0) (2022-11-19)\r\n* [Stream-K](/examples/47_ampere_gemm_universal_streamk), which is a new general way to do split-K.  It can not only improve performance, but can also significantly reduce the number of tile sizes that need to be profiled to find the best one.\r\n* [Fused multi-head attention Kernel](/examples/41_fused_multi_head_attention).  It has two variants: one uses batched GEMM for the fixed sequence length, and the other one uses group GEMM for the variable sequence length.  Both versions just need one kernel.\r\n* [Dual GEMM](/examples/45_dual_gemm), which can fuse A x B and A x C into one kernel. Two GEMMs has no producer-consumer dependency.\r\n* Hopper improves [double precision matrix multiplication](/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm90.cu) by 2x compared to Ampere at iso-clocks. It is supported since CUDA 11.8.\r\n* [BLAS3](/test/unit/gemm/device/hemm_cf64_cf64_cf64_tensor_op_f64_sm90.cu) functions with Hoppers new double precision matrix multiplication instructions.\r\n* [ELL Block Sparse GEMM](/examples/43_ell_block_sparse_gemm), which uses an [ELL matrix](https://developer.nvidia.com/blog/accelerating-matrix-multiplication-with-block-sparse-format-and-nvidia-tensor-cores/) to describe the sparsity of A matrix.  B and output matrices are still dense. The block size can be arbitary.\r\n* Optimized [Group Conv](/examples/42_ampere_tensorop_group_conv) for SingleGroup mode, which requires that the output channel per group is a multiple of Threadblock tile N.\r\n* [Optimized DepthWise Conv](/examples/46_depthwise_simt_conv2dfprop/depthwise_simt_conv2dfprop.cu).  Two new modes are added\r\n  * [kOptimized](/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu) - use direct conv to compute instead of implicit GEMM. \r\n    *  The restrictions are: 1) input ,output channel and group number should be multiple of (128 / sizeof(input element)). 2) The input filter size should be the same as the template parameter configuration.\r\n  * [kFixedStrideDilation](/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_fixed_stride_dilation_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu) - which puts stride and dilation into templates to further improve the performance. In this mode, kernel persistents some inputs into register to squeeze more performance, so large filter/stride/dilation is not recommanded.\r\n    * The restrictions are: 1) input, output channel and group number should be multiple of (128 / sizeof(input element)). 2) input filter size, stride, dilation should same as the template parameter configuration. \r\n* [Scripts](/examples/44_multi_gemm_ir_and_codegen) to fuse multiple back-to-back GEMM.  Its implementation was discussed in a GTC'22 Spring [talk](https://www.nvidia.com/en-us/on-demand/session/gtcspring22-s41606/).\r\n* [FP8 data type definition](/include/cutlass/float8.h) and [conversion routines](/include/cutlass/numeric_conversion.h#L1274-2115).\r\n* Updates and bugfixes from the community (thanks!).  Big shout out to Meta's [xFormers](https://github.com/facebookresearch/xformers).\r\n\r\n* **Deprecation announcement:** CUTLASS plans to deprecate the following:\r\n  * Maxwell and Pascal GPU architectures\r\n  * Ubuntu 16.04\r\n  * CUDA 10.2","reactions":{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/89778027/reactions","total_count":11,"+1":4,"-1":0,"laugh":0,"hooray":4,"confused":0,"heart":3,"rocket":0,"eyes":0}},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/77353063","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/77353063/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/77353063/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v2.10.0","id":77353063,"author":{"login":"hwu36","id":57973641,"node_id":"MDQ6VXNlcjU3OTczNjQx","avatar_url":"https://avatars.githubusercontent.com/u/57973641?v=4","gravatar_id":"","url":"https://api.github.com/users/hwu36","html_url":"https://github.com/hwu36","followers_url":"https://api.github.com/users/hwu36/followers","following_url":"https://api.github.com/users/hwu36/following{/other_user}","gists_url":"https://api.github.com/users/hwu36/gists{/gist_id}","starred_url":"https://api.github.com/users/hwu36/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hwu36/subscriptions","organizations_url":"https://api.github.com/users/hwu36/orgs","repos_url":"https://api.github.com/users/hwu36/repos","events_url":"https://api.github.com/users/hwu36/events{/privacy}","received_events_url":"https://api.github.com/users/hwu36/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"RE_kwDOBrVDM84EnFBn","tag_name":"v2.10.0","target_commitish":"master","name":"CUTLASS 2.10.0","draft":false,"immutable":false,"prerelease":false,"created_at":"2022-09-15T20:20:33Z","published_at":"2022-09-16T02:42:34Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v2.10.0","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v2.10.0","body":"**CUTLASS 2.10.0**\r\n\r\n[CUTLASS Python](https://github.com/NVIDIA/cutlass/blob/master/examples/40_cutlass_py) now supports GEMM, Convolution and Grouped GEMM for different data types as well as different epilogue flavors.\r\nOptimizations for CUTLASS's [Grouped GEMM](https://github.com/NVIDIA/cutlass/blob/master/examples/24_gemm_grouped/gemm_grouped.cu) kernel. It can move some scheduling into the host side if applicable.\r\nOptimizations for [GEMM+Softmax](https://github.com/NVIDIA/cutlass/blob/master/examples/35_gemm_softmax).\r\n[Grouped GEMM for Multihead Attention](https://github.com/NVIDIA/cutlass/blob/master/examples/41_multi_head_attention) is a general MHA that does not require equal sequence length in every GEMM.\r\n[GEMM + Layer norm fusion for Ampere](https://github.com/NVIDIA/cutlass/blob/master/examples/37_gemm_layernorm_gemm_fusion) can fuse the layernorm into GEMMs before and after.\r\n[GEMM Epilogue Permutation Fusion](https://github.com/NVIDIA/cutlass/blob/master/examples/39_gemm_permute) can permute the GEMM output before storing.\r\n[Grouped convolution targeting implicit GEMM](https://github.com/NVIDIA/cutlass/blob/master/test/unit/conv/device/group_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu) introduces the first group convolution implementation to CUTLASS. It is an Analytical implementation, not an Optimized.\r\n[Depthwise separable convolution](https://github.com/NVIDIA/cutlass/blob/master/test/unit/conv/device/depthwise_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu) introduces the first depthwise convolution which is also Analytical for now.\r\nStandalone [Layernorm](https://github.com/NVIDIA/cutlass/blob/master/tools/util/include/cutlass/util/device_layernorm.h) and [Pooling](https://github.com/NVIDIA/cutlass/blob/master/tools/util/include/cutlass/util/device_nhwc_pooling.h) kernels.\r\n[Back-to-back GEMM](https://github.com/NVIDIA/cutlass/blob/master/examples/13_two_tensor_op_fusion) enhancements.\r\nUpdates and bugfixes from the community (thanks!)","discussion_url":"https://github.com/NVIDIA/cutlass/discussions/627"},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/70815367","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/70815367/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/70815367/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v2.9.1","id":70815367,"author":{"login":"kerrmudgeon","id":11259,"node_id":"MDQ6VXNlcjExMjU5","avatar_url":"https://avatars.githubusercontent.com/u/11259?v=4","gravatar_id":"","url":"https://api.github.com/users/kerrmudgeon","html_url":"https://github.com/kerrmudgeon","followers_url":"https://api.github.com/users/kerrmudgeon/followers","following_url":"https://api.github.com/users/kerrmudgeon/following{/other_user}","gists_url":"https://api.github.com/users/kerrmudgeon/gists{/gist_id}","starred_url":"https://api.github.com/users/kerrmudgeon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kerrmudgeon/subscriptions","organizations_url":"https://api.github.com/users/kerrmudgeon/orgs","repos_url":"https://api.github.com/users/kerrmudgeon/repos","events_url":"https://api.github.com/users/kerrmudgeon/events{/privacy}","received_events_url":"https://api.github.com/users/kerrmudgeon/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"RE_kwDOBrVDM84EOI6H","tag_name":"v2.9.1","target_commitish":"master","name":"CUTLASS 2.9.1","draft":false,"immutable":false,"prerelease":false,"created_at":"2022-06-28T11:29:38Z","published_at":"2022-06-29T01:15:32Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v2.9.1","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v2.9.1","body":"Bug fixes, performance tuning, and enhancements to documentation.","reactions":{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/70815367/reactions","total_count":5,"+1":0,"-1":0,"laugh":0,"hooray":5,"confused":0,"heart":0,"rocket":0,"eyes":0}},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/65491232","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/65491232/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/65491232/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v2.9.0","id":65491232,"author":{"login":"kerrmudgeon","id":11259,"node_id":"MDQ6VXNlcjExMjU5","avatar_url":"https://avatars.githubusercontent.com/u/11259?v=4","gravatar_id":"","url":"https://api.github.com/users/kerrmudgeon","html_url":"https://github.com/kerrmudgeon","followers_url":"https://api.github.com/users/kerrmudgeon/followers","following_url":"https://api.github.com/users/kerrmudgeon/following{/other_user}","gists_url":"https://api.github.com/users/kerrmudgeon/gists{/gist_id}","starred_url":"https://api.github.com/users/kerrmudgeon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kerrmudgeon/subscriptions","organizations_url":"https://api.github.com/users/kerrmudgeon/orgs","repos_url":"https://api.github.com/users/kerrmudgeon/repos","events_url":"https://api.github.com/users/kerrmudgeon/events{/privacy}","received_events_url":"https://api.github.com/users/kerrmudgeon/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"RE_kwDOBrVDM84D51Eg","tag_name":"v2.9.0","target_commitish":"master","name":"CUTLASS 2.9.0","draft":false,"immutable":false,"prerelease":false,"created_at":"2022-04-27T14:02:26Z","published_at":"2022-04-27T16:31:03Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v2.9.0","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v2.9.0","body":"**CUTLASS 2.9.0**\r\n* [First layer Convolution kernels](/test/unit/conv/device/conv2d_fprop_fixed_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu) specialized for small channel counts and reduced alignment\r\n  * [Few channels](/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_few_channels.h) specialization for reduced alignment capabilities\r\n  * [Fixed channels](/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_fixed_channels.h) further specialized when channel count perfectly matches the access vector size\r\n  * [Unit tests](/test/unit/conv/device/conv2d_fprop_few_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu)\r\n  * [Python-based instance emitter](/tools/library/scripts/generator.py) in the CUTLASS Library and support in the Profiler\r\n* [BLAS3](https://docs.nvidia.com/cuda/cublas/index.html#cublas-level-3-function-reference) operators accelerated by Tensor Cores\r\n  * Supported types: f32, cf32, f64, cf64\r\n  * [HERK](/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_fast_f32_sm80.cu) with [emitter](/tools/library/scripts/rank_k_operation.py)\r\n  * [SYRK](/test/unit/gemm/device/syrk_f32n_f32t_tensor_op_fast_f32_sm80.cu) with [emitter](/tools/library/scripts/rank_k_operation.py)\r\n  * [SYMM](/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu) with [emitter](/tools/library/scripts/symm_operation.py)\r\n  * [TRMM](/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu) with [emitter](/tools/library/scripts/trmm_operation.py)\r\n  * [Unit tests](/test/unit/gemm/device/testbed_rank_k_universal.h)\r\n* [CUTLASS Python](/example/40_cutlass_py) demonstrating JIT compilation of CUTLASS kernels and a Python-based runtime using [CUDA Python](https://developer.nvidia.com/cuda-python)\r\n  * [Python-based runtime](/tools/library/scripts/rt.py) interoperable with existing emitters\r\n* [GEMM + Softmax example](/examples/35_gemm_softmax)\r\n* Optimal performance using [**CUDA 11.6u2**](https://developer.nvidia.com/cuda-downloads)\r\n* Updates and bugfixes from the community (thanks!)","reactions":{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/65491232/reactions","total_count":6,"+1":0,"-1":0,"laugh":0,"hooray":4,"confused":0,"heart":2,"rocket":0,"eyes":0}},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/54736155","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/54736155/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/54736155/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v2.8.0","id":54736155,"author":{"login":"kerrmudgeon","id":11259,"node_id":"MDQ6VXNlcjExMjU5","avatar_url":"https://avatars.githubusercontent.com/u/11259?v=4","gravatar_id":"","url":"https://api.github.com/users/kerrmudgeon","html_url":"https://github.com/kerrmudgeon","followers_url":"https://api.github.com/users/kerrmudgeon/followers","following_url":"https://api.github.com/users/kerrmudgeon/following{/other_user}","gists_url":"https://api.github.com/users/kerrmudgeon/gists{/gist_id}","starred_url":"https://api.github.com/users/kerrmudgeon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kerrmudgeon/subscriptions","organizations_url":"https://api.github.com/users/kerrmudgeon/orgs","repos_url":"https://api.github.com/users/kerrmudgeon/repos","events_url":"https://api.github.com/users/kerrmudgeon/events{/privacy}","received_events_url":"https://api.github.com/users/kerrmudgeon/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"RE_kwDOBrVDM84DQzUb","tag_name":"v2.8.0","target_commitish":"master","name":"CUTLASS 2.8","draft":false,"immutable":false,"prerelease":false,"created_at":"2021-12-06T19:21:33Z","published_at":"2021-12-06T19:22:54Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v2.8.0","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v2.8.0","body":"* **TF32x3:** emulated single-precision using Tensor Cores\r\n  * 45+ TFLOPs on NVIDIA A100\r\n  * [GEMM SDK example](/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu) (real)\r\n  * [COMPLEX GEMM SDK example](/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm.cu) (complex)\r\n  * [Implicit GEMM Convolution SDK example](/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/ampere_3xtf32_fast_accurate_tensorop_fprop.cu)\r\n* **Mainloop fusion for Convolution:** convolution with fused per-channel scale-bias-relu\r\n  * [Conv Fprop SDK example](/examples/25_ampere_fprop_mainloop_fusion/ampere_fprop_mainloop_fusion.cu)\r\n  * [Conv WGrad SDK example](/examples/26_ampere_wgrad_mainloop_fusion/ampere_wgrad_mainloop_fusion.cu) \r\n  * [cutlass::conv::device::ImplicitGemmConvolutionFusion](/include/cutlass/conv/device/implicit_gemm_convolution_fusion.h)\r\n* **Grouped GEMM:** similar to batched GEMM with distinct problem size per group\r\n  * [SDK example](/examples/24_gemm_grouped) with performance comparison with Batched Strided GEMM\r\n  * [cutlass::gemm::device::GemmGrouped](/include/cutlass/gemm/device/gemm_grouped.h)\r\n* [Implicit GEMM Convolution fusion](/examples/13_two_tensor_op_fusion/) supports staging 1st convolution's output accumulator in the shared memory on Turing. This allows more flexible warp tile sizes and less regsiter pressue.\r\n* Optimal performance using [**CUDA 11.5**](https://developer.nvidia.com/cuda-downloads)\r\n* Updates from the community (thanks!)\r\n\r\n* **Deprecation announcement:** CUTLASS plans to deprecate the following:\r\n  * Maxwell and Pascal GPU architectures\r\n  * Ubuntu 16.04\r\n  * CUDA 10.2","reactions":{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/54736155/reactions","total_count":10,"+1":2,"-1":0,"laugh":0,"hooray":5,"confused":0,"heart":3,"rocket":0,"eyes":0}},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/49930872","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/49930872/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/49930872/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v2.7.0","id":49930872,"author":{"login":"kerrmudgeon","id":11259,"node_id":"MDQ6VXNlcjExMjU5","avatar_url":"https://avatars.githubusercontent.com/u/11259?v=4","gravatar_id":"","url":"https://api.github.com/users/kerrmudgeon","html_url":"https://github.com/kerrmudgeon","followers_url":"https://api.github.com/users/kerrmudgeon/followers","following_url":"https://api.github.com/users/kerrmudgeon/following{/other_user}","gists_url":"https://api.github.com/users/kerrmudgeon/gists{/gist_id}","starred_url":"https://api.github.com/users/kerrmudgeon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kerrmudgeon/subscriptions","organizations_url":"https://api.github.com/users/kerrmudgeon/orgs","repos_url":"https://api.github.com/users/kerrmudgeon/repos","events_url":"https://api.github.com/users/kerrmudgeon/events{/privacy}","received_events_url":"https://api.github.com/users/kerrmudgeon/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"RE_kwDOBrVDM84C-eJ4","tag_name":"v2.7.0","target_commitish":"master","name":"CUTLASS 2.7","draft":false,"immutable":false,"prerelease":false,"created_at":"2021-09-20T18:02:22Z","published_at":"2021-09-20T18:10:26Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v2.7.0","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v2.7.0","body":"## [2.7.0](https://github.com/NVIDIA/cutlass/releases/tag/v2.7.0) \r\n  * Mainloop fusion for GEMM: [summation over A or B](/examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu)\r\n  * [Strided DGRAD (optimized iterators)](/include/cutlass/conv/kernel/default_conv2d_dgrad.h)\r\n  * [Half-precision GELU_taylor activation functions](/include/cutlass/epilogue/thread/activation.h#L196)\r\n    * Use these when accumulation and epilogue compute types are all `cutlass::half_t`\r\n  * Tuning and bug fixes to [fused GEMM + GEMM example](/examples/13_two_tensor_op_fusion/)\r\n  * Support for smaller than 128b aligned Convolutions: [see examples](test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu#L272)\r\n  * Caching of results to accelerate Convolution [unit tests](test/unit/conv/device/cache_testbed_output.h)\r\n    * Can be enabled or disabled by running `cmake .. -DCUTLASS_TEST_ENABLE_CACHED_RESULTS=OFF`\r\n  * Corrections and bug fixes reported by the CUTLASS community\r\n    * Thank you for filing these issues!","reactions":{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/49930872/reactions","total_count":6,"+1":0,"-1":0,"laugh":0,"hooray":4,"confused":0,"heart":2,"rocket":0,"eyes":0}},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/48981877","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/48981877/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/48981877/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v2.6.1","id":48981877,"author":{"login":"kerrmudgeon","id":11259,"node_id":"MDQ6VXNlcjExMjU5","avatar_url":"https://avatars.githubusercontent.com/u/11259?v=4","gravatar_id":"","url":"https://api.github.com/users/kerrmudgeon","html_url":"https://github.com/kerrmudgeon","followers_url":"https://api.github.com/users/kerrmudgeon/followers","following_url":"https://api.github.com/users/kerrmudgeon/following{/other_user}","gists_url":"https://api.github.com/users/kerrmudgeon/gists{/gist_id}","starred_url":"https://api.github.com/users/kerrmudgeon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kerrmudgeon/subscriptions","organizations_url":"https://api.github.com/users/kerrmudgeon/orgs","repos_url":"https://api.github.com/users/kerrmudgeon/repos","events_url":"https://api.github.com/users/kerrmudgeon/events{/privacy}","received_events_url":"https://api.github.com/users/kerrmudgeon/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"MDc6UmVsZWFzZTQ4OTgxODc3","tag_name":"v2.6.1","target_commitish":"master","name":"CUTLASS 2.6.1","draft":false,"immutable":false,"prerelease":false,"created_at":"2021-09-03T17:26:15Z","published_at":"2021-09-03T17:27:11Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v2.6.1","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v2.6.1","body":"  * Arbitrary padding and striding for CUTLASS Strided DGRAD Convolution operator (Analytic Iterators)\r\n  * Tuning for GEMMs fused with partial reductions\r\n  * Corrections and bug fixes reported by the CUTLASS community\r\n    * Thank you for filing these issues!","reactions":{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/48981877/reactions","total_count":3,"+1":3,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0}},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/48977829","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/48977829/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/48977829/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v2.6.0","id":48977829,"author":{"login":"kerrmudgeon","id":11259,"node_id":"MDQ6VXNlcjExMjU5","avatar_url":"https://avatars.githubusercontent.com/u/11259?v=4","gravatar_id":"","url":"https://api.github.com/users/kerrmudgeon","html_url":"https://github.com/kerrmudgeon","followers_url":"https://api.github.com/users/kerrmudgeon/followers","following_url":"https://api.github.com/users/kerrmudgeon/following{/other_user}","gists_url":"https://api.github.com/users/kerrmudgeon/gists{/gist_id}","starred_url":"https://api.github.com/users/kerrmudgeon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kerrmudgeon/subscriptions","organizations_url":"https://api.github.com/users/kerrmudgeon/orgs","repos_url":"https://api.github.com/users/kerrmudgeon/repos","events_url":"https://api.github.com/users/kerrmudgeon/events{/privacy}","received_events_url":"https://api.github.com/users/kerrmudgeon/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"MDc6UmVsZWFzZTQ4OTc3ODI5","tag_name":"v2.6.0","target_commitish":"master","name":"CUTLASS 2.6.0","draft":false,"immutable":false,"prerelease":false,"created_at":"2021-08-08T18:54:42Z","published_at":"2021-09-03T16:52:19Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v2.6.0","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v2.6.0","body":"**CUTLASS 2.6.0**\r\n\r\n  * Optimal performance when compiled with the [CUDA 11.4 Toolkit](https://developer.nvidia.com/cuda-toolkit)\r\n    * Adopt the new L2 prefetch feature in [cp.async](/include/cutlass/arch/memory.h) and [global load](/include/cutlass/arch/memory_sm80.h)\r\n  * Fused operators with GEMM and Convolution\r\n    * [Fused broadcast in epilogue](test/unit/gemm/device/gemm_with_broadcast_f16n_f16n_f16n_tensorop_f32_sm75.cu)\r\n    * [Fused partial reduction in epilogue](/test/unit/gemm/device/gemm_with_reduction_f16n_f16n_f16n_tensorop_f32_sm75.cu)\r\n  * 64b tensor strides and leading dimensions support for GEMMs\r\n  * Affine rank=2 matrix layouts \r\n    * Row stride and column stride for matrices using [cutlass::layout::AffineRank2](/include/cutlass/layout/matrix.h)\r\n    * Support [FP64 tensor core](/examples/18_ampere_fp64_tensorop_affine2_gemm/ampere_fp64_tensorop_affine2_gemm.cu) and SIMT GEMM.\r\n  * [Batched GEMV](/test/unit/gemm/device/gemv.cu) preview implementation\r\n  * [New strided Dgrad](test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu) implementation\r\n    * Accelerates over previous implementation by cutting down redundant math by 4x\r\n    * Support using new `Dy` and `w` analytic iterators and existing `cutlass::conv::device::ImplicitGemmConvolution` interface\r\n  * Quaternion-valued GEMM and Convolution in single- and double-precision (targeting CUDA Cores)\r\n    * Updates to [quaternion.h](/include/cutlass/quaternion.h) and [functional.h](/include/cutlass/functional.h)\r\n    * SDK Example for [GEMM](/examples/21_quaternion_gemm/quaternion_gemm.cu) and [Convolution](/examples/22_quaternion_gemm/quaternion_conv.cu)\r\n    * [Unit tests for GEMM](/test/unit/gemm/device/simt_qgemm_nn_sm50.cu) and [Convolution](/test/unit/conv/device/conv2d_fprop_implicit_gemm_qf32nhwc_qf32nhwc_qf32nhwc_simt_f32_sm50.cu)\r\n  * Many improvements to the epilogue.\r\n    * Provide an [option](/include/cutlass/epilogue/threadblock/epilogue.h) to not fully unroll the epilogue to reduce the code size and improve the performance when using complicated elementwise operations\r\n    * Performance improvement for FP16 tensor core kernels\r\n    * Bug fixes\r\n  * Enhanced Clang support and the combination of Clang 13 and CUDA 11.4 can build and run kernels from Pascal and Ampere.   \r\n  * Updated minimum CUDA Toolkit requirement to 10.2\r\n    * [CUDA 11.4 Toolkit](https://developer.nvidia.com/cuda-toolkit) recommended\r\n  * Corrections and bug fixes reported by the CUTLASS community\r\n    * Thank you for filing these issues!","reactions":{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/48977829/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":1,"confused":0,"heart":0,"rocket":0,"eyes":0}},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/39207381","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/39207381/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/39207381/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v2.5.0","id":39207381,"author":{"login":"kerrmudgeon","id":11259,"node_id":"MDQ6VXNlcjExMjU5","avatar_url":"https://avatars.githubusercontent.com/u/11259?v=4","gravatar_id":"","url":"https://api.github.com/users/kerrmudgeon","html_url":"https://github.com/kerrmudgeon","followers_url":"https://api.github.com/users/kerrmudgeon/followers","following_url":"https://api.github.com/users/kerrmudgeon/following{/other_user}","gists_url":"https://api.github.com/users/kerrmudgeon/gists{/gist_id}","starred_url":"https://api.github.com/users/kerrmudgeon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kerrmudgeon/subscriptions","organizations_url":"https://api.github.com/users/kerrmudgeon/orgs","repos_url":"https://api.github.com/users/kerrmudgeon/repos","events_url":"https://api.github.com/users/kerrmudgeon/events{/privacy}","received_events_url":"https://api.github.com/users/kerrmudgeon/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"MDc6UmVsZWFzZTM5MjA3Mzgx","tag_name":"v2.5.0","target_commitish":"master","name":"CUTLASS 2.5.0","draft":false,"immutable":false,"prerelease":false,"created_at":"2021-03-03T19:17:40Z","published_at":"2021-03-03T19:20:06Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v2.5.0","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v2.5.0","body":"CUTLASS 2.5 is a minor release contributing:\r\n  * Tensor reductions\r\n    * _m_-to-_n_ reductions of tensors with affine layout\r\n    * [Specializations](/test/unit/reduction/device/tensor_reduce_contiguous.cu) for reductions including contiguous dimension\r\n    * [Specializations](/test/unit/reduction/device/tensor_reduce_strided.cu) for reductions excluding contiguous dimension\r\n    * Custom reduction functors such as `cutlass::logical_and`\r\n    * Large tensor support, up to 2^63 elements (however, each dimension is limited to an extent of 2^31)\r\n  * Optimizations for 3-D convolution\r\n    * [Optimized tile iterators](include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_optimized.h) using precomputed delta table for 3-D convolution\r\n    * Full coverage of [forward](test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu) and [backwards](test/unit/conv/device/conv3d_dgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu) passes for 3D convolution\r\n  * [Fused Convolution+Convolution example](/examples/13_two_tensor_op_fusion/README.md)\r\n  * Corrections and bug fixes reported by the CUTLASS community\r\n    * Thank you for filing these issues!"},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/34770637","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/34770637/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/34770637/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v2.4.0","id":34770637,"author":{"login":"kerrmudgeon","id":11259,"node_id":"MDQ6VXNlcjExMjU5","avatar_url":"https://avatars.githubusercontent.com/u/11259?v=4","gravatar_id":"","url":"https://api.github.com/users/kerrmudgeon","html_url":"https://github.com/kerrmudgeon","followers_url":"https://api.github.com/users/kerrmudgeon/followers","following_url":"https://api.github.com/users/kerrmudgeon/following{/other_user}","gists_url":"https://api.github.com/users/kerrmudgeon/gists{/gist_id}","starred_url":"https://api.github.com/users/kerrmudgeon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kerrmudgeon/subscriptions","organizations_url":"https://api.github.com/users/kerrmudgeon/orgs","repos_url":"https://api.github.com/users/kerrmudgeon/repos","events_url":"https://api.github.com/users/kerrmudgeon/events{/privacy}","received_events_url":"https://api.github.com/users/kerrmudgeon/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"MDc6UmVsZWFzZTM0NzcwNjM3","tag_name":"v2.4.0","target_commitish":"master","name":"CUTLASS 2.4.0","draft":false,"immutable":false,"prerelease":false,"created_at":"2020-11-23T12:59:45Z","published_at":"2020-12-03T16:03:53Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v2.4.0","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v2.4.0","body":"## CUTLASS 2.4\r\n  * Implicit GEMM convolution kernels supporting CUDA and Tensor Cores on NVIDIA GPUs\r\n    * Operators: forward (Fprop), backward data gradient (Dgrad), and backward weight gradient (Wgrad) convolution\r\n    * Data type: FP32, complex<FP32>, Tensor Float 32 (TF32), BFloat16 (BF16), Float16, Int4, Int8, Int32\r\n    * Spatial dimensions: 1-D, 2-D, and 3-D\r\n    * Layout: NHWC, NCxHWx\r\n  * Implicit GEMM convolution components: \r\n    * Global memory iterators supporting Fprop, Dgrad, and Wgrad\r\n    * `MmaMultistage` for implicit GEMM convolution for NVIDIA Ampere architecture\r\n    * `MmaPipeline` for implicit GEMM convolution for NVIDIA Volta and Turing architectures\r\n    * [Documentation](/media/docs/implicit_gemm_convolution.md) describing Implicit GEMM Convolution algorithm and implementation"},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/31826296","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/31826296/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/31826296/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v2.3.0","id":31826296,"author":{"login":"kerrmudgeon","id":11259,"node_id":"MDQ6VXNlcjExMjU5","avatar_url":"https://avatars.githubusercontent.com/u/11259?v=4","gravatar_id":"","url":"https://api.github.com/users/kerrmudgeon","html_url":"https://github.com/kerrmudgeon","followers_url":"https://api.github.com/users/kerrmudgeon/followers","following_url":"https://api.github.com/users/kerrmudgeon/following{/other_user}","gists_url":"https://api.github.com/users/kerrmudgeon/gists{/gist_id}","starred_url":"https://api.github.com/users/kerrmudgeon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kerrmudgeon/subscriptions","organizations_url":"https://api.github.com/users/kerrmudgeon/orgs","repos_url":"https://api.github.com/users/kerrmudgeon/repos","events_url":"https://api.github.com/users/kerrmudgeon/events{/privacy}","received_events_url":"https://api.github.com/users/kerrmudgeon/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"MDc6UmVsZWFzZTMxODI2Mjk2","tag_name":"v2.3.0","target_commitish":"master","name":"CUTLASS 2.3","draft":false,"immutable":false,"prerelease":false,"created_at":"2020-09-25T18:25:26Z","published_at":"2020-09-25T18:27:32Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v2.3.0","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v2.3.0","body":"**CUTLASS 2.3**\r\n * [NVIDIA Ampere Architecture features](https://devblogs.nvidia.com/nvidia-ampere-architecture-in-depth/)\r\n   * [Sparse Tensor Core GEMM kernels](test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sparse_sm80.cu):\r\n     * Direct access to Sparse Tensor Cores and maximum performance via [`mma.sp.sync`](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-mma-and-friends)\r\n   * Fast SGEMM targeting GeForce RTX 30-series CUDA Cores\r\n * Minor Features:\r\n   * [Activation functions](/include/cutlass/epilogue/thread/activation.h) such as [GeLU](/include/cutlass/epilogue/thread/linear_combination_gelu.h) and [Sigmoid](/include/cutlass/epilogue/thread/linear_combination_sigmoid.h)\r\n   * Small [matrix](/include/cutlass/matrix.h) and [quaternion](/include/cutlass/quaternion.h) template classes in device code\r\n   * [Floating-point constants](/include/cutlass/constants.h)\r\n * NVIDIA Ampere GPU Architecture examples and documentation:\r\n   * [Tensor Float 32](/examples/14_ampere_tf32_tensorop_gemm/ampere_tf32_tensorop_gemm.cu) and \r\n   * [Sparse Tensor Cores](/examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm.cu)\r\n   * Documentation added on CUTLASS [efficient row-major epilogue](/media/docs/gemm_api.md#efficient-epilogue)"},{"url":"https://api.github.com/repos/NVIDIA/cutlass/releases/27566677","assets_url":"https://api.github.com/repos/NVIDIA/cutlass/releases/27566677/assets","upload_url":"https://uploads.github.com/repos/NVIDIA/cutlass/releases/27566677/assets{?name,label}","html_url":"https://github.com/NVIDIA/cutlass/releases/tag/v2.2.0","id":27566677,"author":{"login":"kerrmudgeon","id":11259,"node_id":"MDQ6VXNlcjExMjU5","avatar_url":"https://avatars.githubusercontent.com/u/11259?v=4","gravatar_id":"","url":"https://api.github.com/users/kerrmudgeon","html_url":"https://github.com/kerrmudgeon","followers_url":"https://api.github.com/users/kerrmudgeon/followers","following_url":"https://api.github.com/users/kerrmudgeon/following{/other_user}","gists_url":"https://api.github.com/users/kerrmudgeon/gists{/gist_id}","starred_url":"https://api.github.com/users/kerrmudgeon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kerrmudgeon/subscriptions","organizations_url":"https://api.github.com/users/kerrmudgeon/orgs","repos_url":"https://api.github.com/users/kerrmudgeon/repos","events_url":"https://api.github.com/users/kerrmudgeon/events{/privacy}","received_events_url":"https://api.github.com/users/kerrmudgeon/received_events","type":"User","user_view_type":"public","site_admin":false},"node_id":"MDc6UmVsZWFzZTI3NTY2Njc3","tag_name":"v2.2.0","target_commitish":"master","name":"CUTLASS 2.2 ","draft":false,"immutable":false,"prerelease":false,"created_at":"2020-06-15T17:47:01Z","published_at":"2020-06-15T17:48:26Z","assets":[],"tarball_url":"https://api.github.com/repos/NVIDIA/cutlass/tarball/v2.2.0","zipball_url":"https://api.github.com/repos/NVIDIA/cutlass/zipball/v2.2.0","body":" * [NVIDIA Ampere Architecture features](https://devblogs.nvidia.com/nvidia-ampere-architecture-in-depth/)\r\n   * Fast Tensor Core operations: \r\n    * Maximum performance via [`mma.sync`](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-mma-and-friends)\r\n    * Tensor Float 32, BFloat16, and double-precision data types\r\n    * Mixed integer data types (int8, int4, bin1)\r\n   * Asynchronous copy for deep software pipelines via [`cp.async`](https://docs.nvidia.com/cuda/parallel-thread-execution)   \r\n   * Described in [GTC 2020 Webinar (SR 21745)](https://developer.nvidia.com/gtc/2020/video/s21745) (free registration required)\r\n * Features:\r\n   * SDK examples showing GEMM fused with bias+relu and fused GEMM+GEMM\r\n   * Complex-valued GEMMs targeting NVIDIA Ampere Tensor Cores in double-precision and Tensor Float 32\r\n   * Gaussian complex GEMMs using 3m complex multiply algorithm\r\n   * Universal GEMM kernel supporting two batch modes and two algorithms for parallel reductions\r\n * Policy updates:\r\n   * [CUDA 11 Toolkit](https://developer.nvidia.com/cuda-toolkit) needed to enable NVIDIA Ampere Architecture features\r\n   * Disabled F16C by default for compatibility - enable on cmake command line with `-DCUTLASS_ENABLE_F16C=ON`"}]